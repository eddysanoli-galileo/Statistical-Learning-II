{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Imports"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import glob\r\n",
    "import logging as smlog\r\n",
    "import os\r\n",
    "import traceback\r\n",
    "\r\n",
    "#from smdataset.abstime import calc_note_beats_and_abs_times\r\n",
    "#from smdataset.parse import parse_sm_txt\r\n",
    "\r\n",
    "_ATTR_REQUIRED = ['offset', 'bpms', 'notes']\r\n",
    "\r\n",
    "import argparse\r\n",
    "from collections import OrderedDict\r\n",
    "import json\r\n",
    "\r\n",
    "json.encoder.FLOAT_REPR = lambda f: ('%.6f' % f)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Util"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# FUNCTION THAT TAKES A DIRECTORY AND LISTS ALL OF ITS SUBDIRECTORIES\r\n",
    "# SIMILAR TO WHAT \"OS.WALK\" DOES.\r\n",
    "def get_subdirs(root, choose=False):\r\n",
    "    subdir_names = sorted(filter(lambda x: os.path.isdir(os.path.join(root, x)), os.listdir(root)))\r\n",
    "    if choose:\r\n",
    "        for i, subdir_name in enumerate(subdir_names):\r\n",
    "            print('{}: {}'.format(i, subdir_name))\r\n",
    "        subdir_idxs = [int(x) for x in input('Which subdir(s)? ').split(',')]\r\n",
    "        subdir_names = [subdir_names[i] for i in subdir_idxs]\r\n",
    "    return subdir_names\r\n",
    "\r\n",
    "get_subdirs(\"E:/Escritorio/Temporal\")\r\n",
    "\r\n",
    "\r\n",
    "# FUNCTION THAT REMOVES SPACES, LEAVES NUMBERS AND LETTERS\r\n",
    "# AND REPLACES ANY OTHER SYMBOL WITH A \"_\"\r\n",
    "def ez_name(x):\r\n",
    "    x = ''.join(x.strip().split())\r\n",
    "    x_clean = []\r\n",
    "    for char in x:\r\n",
    "        if char.isalnum():\r\n",
    "            x_clean.append(char)\r\n",
    "        else:\r\n",
    "            x_clean.append('_')\r\n",
    "    return ''.join(x_clean)\r\n",
    "\r\n",
    "\r\n",
    "ez_name(\"ITG is the best! \")"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'ITGisthebest_'"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Parse TXT"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import logging\r\n",
    "import re\r\n",
    "import traceback\r\n",
    "\r\n",
    "parlog = logging\r\n",
    "\r\n",
    "VALID_PULSES = set([4, 8, 12, 16, 24, 32, 48, 64, 96, 192])\r\n",
    "\r\n",
    "\r\n",
    "int_parser = lambda x: int(x.strip()) if x.strip() else None\r\n",
    "bool_parser = lambda x: True if x.strip() == 'YES' else False\r\n",
    "str_parser = lambda x: x.strip() if x.strip() else None\r\n",
    "float_parser = lambda x: float(x.strip()) if x.strip() else None\r\n",
    "\r\n",
    "\r\n",
    "def kv_parser(k_parser, v_parser):\r\n",
    "    def parser(x):\r\n",
    "        if not x:\r\n",
    "            return (None, None)\r\n",
    "        k, v = x.split('=', 1)\r\n",
    "        return k_parser(k), v_parser(v)\r\n",
    "    return parser\r\n",
    "def list_parser(x_parser):\r\n",
    "    def parser(l):\r\n",
    "        l_strip = l.strip()\r\n",
    "        if len(l_strip) == 0:\r\n",
    "            return []\r\n",
    "        else:\r\n",
    "            return [x_parser(x) for x in l_strip.split(',')]\r\n",
    "    return parser\r\n",
    "\r\n",
    "def bpms_parser(x):\r\n",
    "    bpms = list_parser(kv_parser(float_parser, float_parser))(x)\r\n",
    "\r\n",
    "    if len(bpms) == 0:\r\n",
    "        raise ValueError('No BPMs found in list')\r\n",
    "    if bpms[0][0] != 0.0:\r\n",
    "        raise ValueError('First beat in BPM list is {}'.format(bpms[0][0]))\r\n",
    "\r\n",
    "    # make sure changes are nonnegative, take last for equivalent\r\n",
    "    beat_last = -1.0\r\n",
    "    bpms_cleaned = []\r\n",
    "    for beat, bpm in bpms:\r\n",
    "        if beat == None or bpm == None:\r\n",
    "            raise ValueError('Empty BPM found')\r\n",
    "        if bpm <= 0.0:\r\n",
    "            raise ValueError('Non positive BPM found {}'.format(bpm))\r\n",
    "        if beat == beat_last:\r\n",
    "            bpms_cleaned[-1] = (beat, bpm)\r\n",
    "            continue\r\n",
    "        bpms_cleaned.append((beat, bpm))\r\n",
    "        if beat <= beat_last:\r\n",
    "            raise ValueError('Descending list of beats in BPM list')\r\n",
    "        beat_last = beat\r\n",
    "    if len(bpms) != len(bpms_cleaned):\r\n",
    "        parlog.warning('One or more (beat, BPM) pairs begin on the same beat, using last listed')\r\n",
    "\r\n",
    "    return bpms_cleaned\r\n",
    "def stops_parser(x):\r\n",
    "    stops = list_parser(kv_parser(float_parser, float_parser))(x)\r\n",
    "\r\n",
    "    beat_last = -1.0\r\n",
    "    for beat, stop_len in stops:\r\n",
    "        if beat == None or stop_len == None:\r\n",
    "            raise ValueError('Bad stop formatting')\r\n",
    "        if beat < 0.0:\r\n",
    "            raise ValueError('Bad beat in stop')\r\n",
    "        if stop_len == 0.0:\r\n",
    "            continue\r\n",
    "        if beat <= beat_last:\r\n",
    "            raise ValueError('Nonascending list of beats in stops')\r\n",
    "        beat_last = beat\r\n",
    "    return stops\r\n",
    "\r\n",
    "def notes_parser(x):\r\n",
    "    pattern = r'([^:]*):' * 5 + r'([^;:]*)'\r\n",
    "    notes_split = re.findall(pattern, x)\r\n",
    "    if len(notes_split) != 1:\r\n",
    "        raise ValueError('Bad formatting of notes section')\r\n",
    "    notes_split = notes_split[0]\r\n",
    "    if (len(notes_split) != 6):\r\n",
    "        raise ValueError('Bad formatting within notes section')\r\n",
    "\r\n",
    "    # parse/clean measures\r\n",
    "    measures = [measure.splitlines() for measure in notes_split[5].split(',')]\r\n",
    "    measures_clean = []\r\n",
    "    for measure in measures:\r\n",
    "        measure_clean = list(filter(lambda pulse: not pulse.strip().startswith('//') and len(pulse.strip()) > 0, measure))\r\n",
    "        measures_clean.append(measure_clean)\r\n",
    "    if len(measures_clean) > 0 and len(measures_clean[-1]) == 0:\r\n",
    "        measures_clean = measures_clean[:-1]\r\n",
    "\r\n",
    "    # check measure lengths\r\n",
    "    for measure in measures_clean:\r\n",
    "        if len(measure) == 0:\r\n",
    "            raise ValueError('Found measure with 0 notes')\r\n",
    "        if not len(measure) in VALID_PULSES:\r\n",
    "            parlog.warning('Nonstandard subdivision {} detected, allowing'.format(len(measure)))\r\n",
    "\r\n",
    "    chart_type = str_parser(notes_split[0])\r\n",
    "    if chart_type not in ['dance-single', 'dance-double', 'dance-couple', 'lights-cabinet']:\r\n",
    "        raise ValueError('Nonstandard chart type {} detected'.format(chart_type))\r\n",
    "\r\n",
    "    return (str_parser(notes_split[0]),\r\n",
    "        str_parser(notes_split[1]),\r\n",
    "        str_parser(notes_split[2]),\r\n",
    "        int_parser(notes_split[3]),\r\n",
    "        list_parser(float_parser)(notes_split[4]),\r\n",
    "        measures_clean\r\n",
    "    )\r\n",
    "\r\n",
    "def unsupported_parser(attr_name):\r\n",
    "    def parser(x):\r\n",
    "        raise ValueError('Unsupported attribute: {} with value {}'.format(attr_name, x))\r\n",
    "        return None\r\n",
    "    return parser\r\n",
    "\r\n",
    "ATTR_NAME_TO_PARSER = {\r\n",
    "    'title': str_parser,\r\n",
    "    'subtitle': str_parser,\r\n",
    "    'artist': str_parser,\r\n",
    "    'titletranslit': str_parser,\r\n",
    "    'subtitletranslit': str_parser,\r\n",
    "    'artisttranslit': str_parser,\r\n",
    "    'genre': str_parser,\r\n",
    "    'credit': str_parser,\r\n",
    "    'banner': str_parser,\r\n",
    "    'background': str_parser,\r\n",
    "    'lyricspath': str_parser,\r\n",
    "    'cdtitle': str_parser,\r\n",
    "    'music': str_parser,\r\n",
    "    'offset': float_parser,\r\n",
    "    'bpms': bpms_parser,\r\n",
    "    'stops': stops_parser,\r\n",
    "    'samplestart': float_parser,\r\n",
    "    'samplelength': float_parser,\r\n",
    "    'displaybpm': str_parser,\r\n",
    "    'selectable': bool_parser,\r\n",
    "    'bgchanges': str_parser,\r\n",
    "    'bgchanges2': str_parser,\r\n",
    "    'fgchanges': str_parser,\r\n",
    "    'keysounds': str_parser,\r\n",
    "    'musiclength': float_parser,\r\n",
    "    'musicbytes': int_parser,\r\n",
    "    'attacks': str_parser,\r\n",
    "    'timesignatures': list_parser(kv_parser(float_parser, kv_parser(int_parser, int_parser))),\r\n",
    "    'warps': unsupported_parser('warps'),\r\n",
    "    'notes': notes_parser\r\n",
    "}\r\n",
    "\r\n",
    "# LIST OF ATTRIBUTES\r\n",
    "ATTR_MULTI = ['notes']\r\n",
    "\r\n",
    "def parse_sm_txt(sm_txt):\r\n",
    "\r\n",
    "    # CREATES A DICTIONARY WITH AN EMPTY LIST AS VALUE FOR EVERY\r\n",
    "    # ATTRIBUTE IN \"ATTR_MULTI\"\r\n",
    "    attrs = {attr_name: [] for attr_name in ATTR_MULTI}\r\n",
    "\r\n",
    "    # FINDS ALL STRINGS WITH THE SHAPE \"NAME:VALUE\"\r\n",
    "    # THE RESULTS ARE SPLIT INTO THE NAME OF THE ATTRIBUTE AND THE ATTRIBUTE VALUE\r\n",
    "    for attr_name, attr_val in re.findall(r'#([^:]*):([^;]*);', sm_txt):\r\n",
    "\r\n",
    "        # THE ATTRIBUTE NAME IS TURNED TO LOWERCASE\r\n",
    "        attr_name = attr_name.lower()\r\n",
    "\r\n",
    "        # CHECK IF THE ATTRIBUTE IS IN THE DICTIONARY ASSIGNING EACH\r\n",
    "        # ATTRIBUTE TO A PARSER. IF AN ATTRIBUTE IS NOT SUPPORTED A MESSAGE\r\n",
    "        # WILL APPEAR.\r\n",
    "        if attr_name not in ATTR_NAME_TO_PARSER:\r\n",
    "            parlog.warning('Found unexpected attribute {}:{}, ignoring'.format(attr_name, attr_val))\r\n",
    "            continue\r\n",
    "        \r\n",
    "        # PROCESS THE ATTR WITH ITS CORRESPONDING PARSER\r\n",
    "        attr_val_parsed = ATTR_NAME_TO_PARSER[attr_name](attr_val)\r\n",
    "\r\n",
    "        # IF THE ATTRIBUTE IS IN THE LIST OF PREVIOUS ATTRIBUTES\r\n",
    "        if attr_name in attrs:\r\n",
    "\r\n",
    "            # IF THE NAME IS NOT IN THE DICTIONARY OF INITIAL ATTRIBUTES\r\n",
    "            # (CHECK FOR DUPLICATES FOR EVERY ATTRIBUTE EXCEPT NOTES\r\n",
    "            # AS THERE ARE MANY IN A SINGLE FILE)\r\n",
    "            if attr_name not in ATTR_MULTI:\r\n",
    "\r\n",
    "                # IF THE VALUE IN THE DICTIONARY OF ATTRIBUTES FOR THE CURRENT ATTRIBUTE\r\n",
    "                # IS EQUAL TO THE PARSED VALUE, CONTINUE TO THE NEXT ATTRIBUTE \r\n",
    "                # (PREVENTS UNNECESARY OVERWRITES)\r\n",
    "                if attr_val_parsed == attrs[attr_name]:\r\n",
    "                    continue\r\n",
    "\r\n",
    "                # IF TWO DIFFERENT VALUES ARE DETECTED FOR A SINGLE\r\n",
    "                # ATTRIBUTE AN ERROR IS RAISED\r\n",
    "                else:\r\n",
    "                    raise ValueError('Attribute {} defined multiple times'.format(attr_name))\r\n",
    "\r\n",
    "            # ADD NEW PARSED VALUE TO THE CURRENT VALUE STORED\r\n",
    "            attrs[attr_name].append(attr_val_parsed)\r\n",
    "\r\n",
    "        else:\r\n",
    "            # IF THERE WERE NO VALUES FOR THE CURRENT ATTRIBUTE \r\n",
    "            # ADD A VALUE FOR THE KEY \"ATTRIBUTE NAME\"\r\n",
    "            attrs[attr_name] = attr_val_parsed\r\n",
    "\r\n",
    "\r\n",
    "    # CLEAN OR DELETE EMPTY ATTRIBUTES\r\n",
    "    for attr_name, attr_val in list(attrs.items()):\r\n",
    "        if attr_val == None or attr_val == []:\r\n",
    "            del attrs[attr_name]\r\n",
    "\r\n",
    "    return attrs\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Postprocessing of Notes"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "_EPSILON = 1e-6\r\n",
    "\r\n",
    "def bpm_to_spb(bpm):\r\n",
    "    return 60.0 / bpm\r\n",
    "\r\n",
    "def calc_segment_lengths(bpms):\r\n",
    "\r\n",
    "    # CHECK IF THE BPM IS NOT EMPTY\r\n",
    "    assert len(bpms) > 0\r\n",
    "\r\n",
    "    # EMPTY LIST FOR ALL LENGTHS\r\n",
    "    segment_lengths = []\r\n",
    "\r\n",
    "    # COUNTER FROM 0 TO THE TOTAL NUMBER OF BPMS - 2\r\n",
    "    # (SECTIONS WITHOUT A CHANGING BPM)\r\n",
    "    for i in range(len(bpms) - 1):\r\n",
    "\r\n",
    "        # SECONDS PER BEAT\r\n",
    "        spb = bpm_to_spb(bpms[i][1])\r\n",
    "\r\n",
    "        # USES THE \"BEAT\" PART OF THE BPM ATTRIBUTE TO CALCULATE THE \r\n",
    "        # THE NUMBER OF SECONDS IN A SEGMENT WITHOUT CHANGES IN BPM\r\n",
    "        segment_lengths.append(spb * (bpms[i + 1][0] - bpms[i][0]))\r\n",
    "\r\n",
    "\r\n",
    "    return segment_lengths\r\n",
    "\r\n",
    "def calc_abs_for_beat(offset, bpms, stops, segment_lengths, beat):\r\n",
    "    # OFFSET: VALUE\r\n",
    "    # BPMS: LIST OF TUPLES\r\n",
    "    # STOPS: LIST OF TUPLES\r\n",
    "    # SEGMENT_LENGTHS: LIST OF LENGTHS IN SECONDS FOR EACH CHANGE IN BPM\r\n",
    "    # BEAT: CURRENT BEAT\r\n",
    "\r\n",
    "    # BPM INITIAL INDEX\r\n",
    "    bpm_idx = 0\r\n",
    "\r\n",
    "    # FOR EVERY BEAT TIMESTAMP IN \"BPMS\" CHECK IF THE CURRENT BEAT\r\n",
    "    # (PLUS A SMALL DELTA) IS BIGGER THAN THE TIMESTAMP. BASICALLY \r\n",
    "    # CHECK TO WHICH BPM SEGMENT EACH BEAT PERTAINS\r\n",
    "    while bpm_idx < len(bpms) and beat + _EPSILON > bpms[bpm_idx][0]:\r\n",
    "        bpm_idx += 1\r\n",
    "    \r\n",
    "    bpm_idx -= 1\r\n",
    "\r\n",
    "    # CUMULATIVE STOP LENGTH\r\n",
    "    stop_len_cumulative = 0.0\r\n",
    "    #print(\"Stops:\", stops)\r\n",
    "\r\n",
    "    # FOR EACH STOP IN THE SONG\r\n",
    "    for stop_beat, stop_len in stops:\r\n",
    "        #print(f\"Stop Beat: {stop_beat} | Stop Len: {stop_len} | Cumulative: {stop_len_cumulative}\")\r\n",
    "\r\n",
    "        # DISTANCE IN BEATS BETWEEN CURRENT BEAT AND BEAT IN WHICH A STOP OCCURS\r\n",
    "        diff = beat - stop_beat\r\n",
    "\r\n",
    "        # IF THE DIFFERENCE IS TOO LOW, WE ARE AT THE BEAT STOP\r\n",
    "        # We are at this stop which should not count to its timing\r\n",
    "        if abs(diff) < _EPSILON:\r\n",
    "            break\r\n",
    "\r\n",
    "        # We are before this stop\r\n",
    "        elif diff < 0:\r\n",
    "            break\r\n",
    "\r\n",
    "        # We are AFTER this stop\r\n",
    "        # WE ADD TO THE CUMULATIVE SUM OF STOPS\r\n",
    "        else:\r\n",
    "            stop_len_cumulative += stop_len\r\n",
    "        \r\n",
    "\r\n",
    "    # WE TAKE ALL THE LENGTHS BEFORE \"BPM_IDX\" AND WE SUM THEIR LENGTH IN SECONDS\r\n",
    "    # TOTAL LENGTH OF ALL SEGMENTS\r\n",
    "    full_segment_total = sum(segment_lengths[:bpm_idx])\r\n",
    "\r\n",
    "    # GETS THE BPM TIMESTAMP \r\n",
    "    partial_segment_spb = bpm_to_spb(bpms[bpm_idx][1])\r\n",
    "    partial_segment = partial_segment_spb * (beat - bpms[bpm_idx][0])\r\n",
    "\r\n",
    "    #print(\"Cumulative BPM:\", full_segment_total)\r\n",
    "    #print(\"Cumulative Stop:\", stop_len_cumulative)\r\n",
    "    #print(\"Offset:\", offset)\r\n",
    "    #print(\"Partial Segment:\", partial_segment)\r\n",
    "    #print(\"Abs Time:\", full_segment_total + partial_segment - offset + stop_len_cumulative)\r\n",
    "    #print(\"========================\")\r\n",
    "\r\n",
    "    return full_segment_total + partial_segment - offset + stop_len_cumulative\r\n",
    "\r\n",
    "def calc_note_beats_and_abs_times(offset, bpms, stops, note_data):\r\n",
    "\r\n",
    "    # CALCULATE THE LENGTH OF EACH SEGMENT\r\n",
    "    segment_lengths = calc_segment_lengths(bpms)\r\n",
    "\r\n",
    "    # ====================\r\n",
    "\r\n",
    "    # copy bpms\r\n",
    "    bpms = bpms[:]\r\n",
    "    inc = None\r\n",
    "    inc_prev = None\r\n",
    "    time = offset\r\n",
    "\r\n",
    "    # beat loop\r\n",
    "    # INITIAL LISTS\r\n",
    "    note_beats_abs_times = []\r\n",
    "    beat_times = []\r\n",
    "\r\n",
    "    # FOR EACH MEASURE IN THE NOTE DATA\r\n",
    "    for measure_num, measure in enumerate(note_data):\r\n",
    "\r\n",
    "        # LINES IN A MEASURE\r\n",
    "        # (PORTIONS PER MEASURE)\r\n",
    "        ppm = len(measure)\r\n",
    "\r\n",
    "        # FOR EVERY LINE IN A MEASURE\r\n",
    "        for i, code in enumerate(measure):\r\n",
    "            \r\n",
    "            # BEAT = 4*(MEASURE NUMBER) + 4*(LINE NUMBER / LINES IN A MEASURE)\r\n",
    "            # NOTE: THERE ARE 4 BEATS PER MEASURE\r\n",
    "            beat = measure_num * 4.0 + 4.0 * (float(i) / ppm)\r\n",
    "\r\n",
    "            # TODO: This could be much more efficient but is not the bottleneck for the moment.\r\n",
    "            \r\n",
    "            # BEAT IN ABSOLUTE TIME\r\n",
    "            beat_abs = calc_abs_for_beat(offset, bpms, stops, segment_lengths, beat)\r\n",
    "\r\n",
    "\r\n",
    "            note_beats_abs_times.append(((measure_num, ppm, i), beat, beat_abs, code))\r\n",
    "            beat_times.append(beat_abs)\r\n",
    "\r\n",
    "    # handle negative stops\r\n",
    "    beat_time_prev = float('-inf')\r\n",
    "    del_idxs = []\r\n",
    "    for i, beat_time in enumerate(beat_times):\r\n",
    "        if beat_time_prev > beat_time:\r\n",
    "            del_idxs.append(i)\r\n",
    "        else:\r\n",
    "            beat_time_prev = beat_time\r\n",
    "    for del_idx in sorted(del_idxs, reverse=True):\r\n",
    "        del note_beats_abs_times[del_idx]\r\n",
    "        del beat_times[del_idx]\r\n",
    "\r\n",
    "    #TODO: remove when stable\r\n",
    "    assert sorted(beat_times) == beat_times\r\n",
    "\r\n",
    "    return note_beats_abs_times\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Command Line Argument Parsing\r\n",
    "\r\n",
    "**DONT EXECUTE: PARSING FOR THE COMMAND LINE ONLY**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import argparse\r\n",
    "from collections import OrderedDict\r\n",
    "import json\r\n",
    "\r\n",
    "json.encoder.FLOAT_REPR = lambda f: ('%.6f' % f)\r\n",
    "\r\n",
    "parser = argparse.ArgumentParser()\r\n",
    "parser.add_argument('packs_dir', type=str, help='Directory of packs (organized like Stepmania songs folder)')\r\n",
    "parser.add_argument('json_dir', type=str, help='Output JSON directory')\r\n",
    "parser.add_argument('--itg', dest='itg', action='store_true', help='If set, subtract 9ms from offset')\r\n",
    "parser.add_argument('--choose', dest='choose', action='store_true', help='If set, choose from list of packs')\r\n",
    "\r\n",
    "parser.set_defaults(\r\n",
    "    itg=False,\r\n",
    "    choose=False)\r\n",
    "\r\n",
    "args = parser.parse_args()\r\n",
    "\r\n",
    "pack_names = get_subdirs(args.packs_dir, args.choose)\r\n",
    "pack_dirs = [os.path.join(args.packs_dir, pack_name) for pack_name in pack_names]\r\n",
    "pack_sm_globs = [os.path.join(pack_dir, '*', '*.sm') for pack_dir in pack_dirs]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Overwriting Inputs"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "pack_names = [\"fraxtil\", \"itg\", \"kda\"]\r\n",
    "pack_dirs = [\"data/raw/fraxtil\", \"data/raw/itg\", \"data/raw/kda\"]\r\n",
    "json_dir = \"./data/json_raw\"\r\n",
    "\r\n",
    "# GENERIC PATHS FOR ANY SONG FILE WITH EXTENSION \".SM\"\r\n",
    "pack_sm_globs = [os.path.join(pack_dir, \"*\", \"*\", \"*.sm\") for pack_dir in pack_dirs]\r\n",
    "pack_sm_globs"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['data/raw/fraxtil\\\\*\\\\*\\\\*.sm',\n",
       " 'data/raw/itg\\\\*\\\\*\\\\*.sm',\n",
       " 'data/raw/kda\\\\*\\\\*\\\\*.sm']"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## \"Main Loop\""
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# IF THE JSON DIRECTORY DOESNT EXIST, CREATE IT\r\n",
    "if not os.path.isdir(json_dir):\r\n",
    "    os.mkdir(json_dir)\r\n",
    "    print(\"JSON RAW CREATED\")\r\n",
    "\r\n",
    "# SET OF \"EZNAMETS\" FOR A PACK\r\n",
    "pack_eznames = set()\r\n",
    "\r\n",
    "# GLOBS AND NAMES ARE \"ZIPPED\" INTO A TUPLE. \r\n",
    "# ITERATE OVER EVERY PACK NAME, GLOB PAIR.\r\n",
    "for pack_name, pack_sm_glob in zip(pack_names, pack_sm_globs):\r\n",
    "\r\n",
    "    print(f\"PACK NAME: {pack_name}, PACK GLOB: {pack_sm_glob}\")\r\n",
    "\r\n",
    "    # EXTRACT ALL FILES INSIDE THE PACK THAT END IN .SM\r\n",
    "    pack_sm_fps = sorted(glob.glob(pack_sm_glob))\r\n",
    "    \r\n",
    "    # CLEAN THE NAME OF THE PACK\r\n",
    "    pack_ezname = ez_name(pack_name)\r\n",
    "\r\n",
    "    # IF PACK WAS ALREADY CHECKED, RAISE ERROR\r\n",
    "    if pack_ezname in pack_eznames:\r\n",
    "        raise ValueError('Pack name conflict: {}'.format(pack_ezname))\r\n",
    "\r\n",
    "    # PACK IS ADDED TO LIST OF PACKS CHECKED\r\n",
    "    pack_eznames.add(pack_ezname)\r\n",
    "\r\n",
    "    # IF THERE ARE SONGS IN A PACK, CREATE A PATH FOR A DIRECTORY INSIDE \"JSON RAW\"\r\n",
    "    if len(pack_sm_fps) > 0:\r\n",
    "        pack_outdir = os.path.join(json_dir, pack_ezname)\r\n",
    "\r\n",
    "    # IF THE DIRECTORY CORRESPONDING TO THE PATH CREATED DOESNT EXIST, CREATE IT\r\n",
    "    if not os.path.isdir(pack_outdir):\r\n",
    "        os.mkdir(pack_outdir)\r\n",
    "\r\n",
    "    # SET FOR THE NAME OF EACH SONG\r\n",
    "    sm_eznames = set()\r\n",
    "\r\n",
    "    # ITERATE OVER EVERY \".SM\" FILE PATH\r\n",
    "    for sm_fp in pack_sm_fps:\r\n",
    "\r\n",
    "        # EXTRACTS THE NAME OF THE SM FILE (WITHOUT EXTENSION)\r\n",
    "        sm_name = os.path.split(os.path.split(sm_fp)[0])[1]\r\n",
    "\r\n",
    "        #print(\"Song:\", sm_name)\r\n",
    "\r\n",
    "        # THE NAME OF THE SM FILE IS CLEANED\r\n",
    "        sm_ezname = ez_name(sm_name)\r\n",
    "\r\n",
    "        # IF THE SONG WAS PREVIOUSLY PROCESSED\r\n",
    "        if sm_ezname in sm_eznames:\r\n",
    "            raise ValueError('Song name conflict: {}'.format(sm_ezname))\r\n",
    "\r\n",
    "        # IF NO ERROR WAS RAISED, THE SONG NAME IS ADDED\r\n",
    "        sm_eznames.add(sm_ezname)\r\n",
    "\r\n",
    "        # THE TEXT OF THE \".SM\" FILE IS EXTRACTED\r\n",
    "        with open(sm_fp, 'r') as sm_f:\r\n",
    "            sm_txt = sm_f.read()\r\n",
    "\r\n",
    "        # TRY TO PARSE THE FILE\r\n",
    "        try:\r\n",
    "            sm_attrs = parse_sm_txt(sm_txt)\r\n",
    "        except ValueError as e:\r\n",
    "            smlog.error('{} in\\n{}'.format(e, sm_fp))\r\n",
    "            continue\r\n",
    "        except Exception as e:\r\n",
    "            smlog.critical('Unhandled parse exception {}'.format(traceback.format_exc()))\r\n",
    "            raise e\r\n",
    "\r\n",
    "        print(sm_attrs[\"bpms\"], sm_attrs[\"stops\"])\r\n",
    "\r\n",
    "        # CHECKS IF ALL REQUIRED ATTRIBUTES ARE PRESENT IN THE DICTIONARY\r\n",
    "        try:\r\n",
    "            for attr_name in _ATTR_REQUIRED:\r\n",
    "                if attr_name not in sm_attrs:\r\n",
    "                    raise ValueError('Missing required attribute {}'.format(attr_name))\r\n",
    "        except ValueError as e:\r\n",
    "            smlog.error('{}'.format(e))\r\n",
    "            continue\r\n",
    "\r\n",
    "        # HANDLE SONGS WITHOUT MUSIC\r\n",
    "        root = os.path.abspath(os.path.join(sm_fp, '..'))\r\n",
    "        music_fp = os.path.join(root, sm_attrs.get('music', ''))\r\n",
    "        if 'music' not in sm_attrs or not os.path.exists(music_fp):\r\n",
    "            music_names = []\r\n",
    "            sm_prefix = os.path.splitext(sm_name)[0]\r\n",
    "\r\n",
    "            # check directory files for reasonable substitutes\r\n",
    "            for filename in os.listdir(root):\r\n",
    "                prefix, ext = os.path.splitext(filename)\r\n",
    "                if ext.lower()[1:] in ['mp3', 'ogg']:\r\n",
    "                    music_names.append(filename)\r\n",
    "\r\n",
    "            try:\r\n",
    "                # handle errors\r\n",
    "                if len(music_names) == 0:\r\n",
    "                    raise ValueError('No music files found')\r\n",
    "                elif len(music_names) == 1:\r\n",
    "                    sm_attrs['music'] = music_names[0]\r\n",
    "                else:\r\n",
    "                    raise ValueError('Multiple music files {} found'.format(music_names))\r\n",
    "            except ValueError as e:\r\n",
    "                smlog.error('{}'.format(e))\r\n",
    "                continue\r\n",
    "\r\n",
    "            music_fp = os.path.join(root, sm_attrs['music'])\r\n",
    "\r\n",
    "        # EXTRACTS THE VALUE OF THE STOPS, OFFSET AND STOPS\r\n",
    "        bpms = sm_attrs['bpms']\r\n",
    "        offset = sm_attrs['offset']\r\n",
    "        itg = False\r\n",
    "\r\n",
    "        if itg:\r\n",
    "            # Many charters add 9ms of delay to their stepfiles to account for ITG r21/r23 global delay\r\n",
    "            # see http://r21freak.com/phpbb3/viewtopic.php?f=38&t=12750\r\n",
    "            offset -= 0.009\r\n",
    "\r\n",
    "        stops = sm_attrs.get('stops', [])\r\n",
    "\r\n",
    "\r\n",
    "        # DICTIONARY THAT STORES ALL THE INFO THAT WILL BE PUT INTO THE OUTPUT JSOON\r\n",
    "        out_json_fp = os.path.join(pack_outdir, '{}_{}.json'.format(pack_ezname, sm_ezname))\r\n",
    "        out_json = OrderedDict([\r\n",
    "            ('sm_fp', os.path.abspath(sm_fp)),\r\n",
    "            ('music_fp', os.path.abspath(music_fp)),\r\n",
    "            ('pack', pack_name),\r\n",
    "            ('title', sm_attrs.get('title')),\r\n",
    "            ('artist', sm_attrs.get('artist')),\r\n",
    "            ('offset', offset),\r\n",
    "            ('bpms', bpms),\r\n",
    "            ('stops', stops),\r\n",
    "            ('charts', [])\r\n",
    "        ])\r\n",
    "\r\n",
    "        # FOR EVERY ELEMENT INSIDE THE \"NOTES ATTRIBUTE\"\r\n",
    "        # (THIS INCLUDES THE DIFFICULTY, CHART TYPE, GROOVE METER AND CHART NOTES)\r\n",
    "        for idx, sm_notes in enumerate(sm_attrs['notes']):\r\n",
    "\r\n",
    "            # CHART NOTES ARE POST PROCESSED\r\n",
    "            note_beats_and_abs_times = calc_note_beats_and_abs_times(offset, bpms, stops, sm_notes[5])\r\n",
    "\r\n",
    "\r\n",
    "            # note_beats_abs_times.append(((measure_num, ppm, i), beat, beat_abs, code))\r\n",
    "            notes = {\r\n",
    "                'type': sm_notes[0],\r\n",
    "                'desc_or_author': sm_notes[1],\r\n",
    "                'difficulty_coarse': sm_notes[2],\r\n",
    "                'difficulty_fine': sm_notes[3],\r\n",
    "                'notes': note_beats_and_abs_times,\r\n",
    "            }\r\n",
    "            out_json['charts'].append(notes)\r\n",
    "\r\n",
    "        with open(out_json_fp, 'w') as out_f:\r\n",
    "            try:\r\n",
    "                out_f.write(json.dumps(out_json))\r\n",
    "            except UnicodeDecodeError:\r\n",
    "                smlog.error('Unicode error in {}'.format(sm_fp))\r\n",
    "                continue\r\n",
    "\r\n",
    "        print('Parsed {} - {}: {} charts'.format(pack_name, sm_name, len(out_json['charts'])))\r\n",
    "    \r\n",
    "    break"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "PACK NAME: fraxtil, PACK GLOB: data/raw/fraxtil\\*\\*\\*.sm\n",
      "Song: Bad Ketchup\n",
      "[(0.0, 180.0)] [(0.0, 180.0)]\n",
      "Parsed fraxtil - Bad Ketchup: 9 charts\n",
      "Song: Bitch Clap\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyError",
     "evalue": "'stops'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-9bc9bbeeb562>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msm_attrs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"bpms\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msm_attrs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"stops\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[1;31m# CHECKS IF ALL REQUIRED ATTRIBUTES ARE PRESENT IN THE DICTIONARY\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'stops'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.2",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.2 64-bit"
  },
  "interpreter": {
   "hash": "658dc12c475a3a8caebf03b24f414cffa2901ebd330ffd26b9c22f028a90850c"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}