{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Dance Dance Convolution - Revisited\r\n",
    "\r\n",
    "Revision of the original \"Dance Dance Convolution\" paper, that incorporates newer machine learning and AI techniques to \"hopefully\" improve the original model results."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Imports"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "source": [
    "import os\r\n",
    "import sys\r\n",
    "import pickle\r\n",
    "import logging\r\n",
    "from tqdm import tqdm\r\n",
    "from pathlib import Path\r\n",
    "from collections import defaultdict\r\n",
    "from os.path import isfile, join, splitext, basename, normpath, exists\r\n",
    "\r\n",
    "import numpy as np\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import librosa\r\n",
    "import librosa.display\r\n",
    "\r\n",
    "# Weights and Biases (WandB)\r\n",
    "import wandb \r\n",
    "from wandb.keras import WandbCallback\r\n",
    "\r\n",
    "# Keras\r\n",
    "# IMPORTANT: Do not mix tensorflow and keras imports for layers or optimizers.\r\n",
    "import keras\r\n",
    "from tensorflow.keras.utils import to_categorical\r\n",
    "from keras.optimizers import adam_v2\r\n",
    "from keras.models import Sequential\r\n",
    "from keras.layers import Dense\r\n",
    "from keras.layers import Conv2D\r\n",
    "from keras.layers import MaxPool2D\r\n",
    "from keras.layers import Flatten\r\n",
    "from keras.layers import Input\r\n",
    "from keras.layers import Bidirectional\r\n",
    "from keras.layers import LSTM\r\n",
    "from keras.layers import Concatenate\r\n",
    "from keras.layers import TimeDistributed\r\n",
    "from keras.models import Model\r\n",
    "from keras.layers import Dropout\r\n",
    "\r\n",
    "# Custom Functions\r\n",
    "from sm_parsing import stepfile_parser\r\n",
    "from post_process import add_measure_timestamps, log_spectrogram"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loading and Cleaning Dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# Path to dataset\r\n",
    "base_path = \"./dataset\"\r\n",
    "\r\n",
    "# Extension of the required stepfile\r\n",
    "stepfile_ext = \".sm\"\r\n",
    "\r\n",
    "# Allowed audio and stepfile extensions\r\n",
    "audio_exts = [\".ogg\", \".mp3\", \".wav\"]\r\n",
    "steps_exts = [\".sm\", \".ssc\"]\r\n",
    "\r\n",
    "# Song packs inside dataset\r\n",
    "# (Ignores files that are not directories)\r\n",
    "song_packs = [f for f in os.listdir(base_path) if not isfile(join(base_path, f))]\r\n",
    "\r\n",
    "print(f\"Found a total of {len(song_packs)} song packs.\")\r\n",
    "\r\n",
    "# =======================================\r\n",
    "# SONG PACK CLEANING AND DATA EXTRACTION\r\n",
    "# =======================================\r\n",
    "\r\n",
    "# Files that dont add anything to training are deleted (videos, images, txts, etc.)\r\n",
    "# Wanted file extensions\r\n",
    "wanted_ext = audio_exts + steps_exts\r\n",
    "\r\n",
    "# Counter for the number of files deleted.\r\n",
    "files_deleted = 0\r\n",
    "\r\n",
    "# Counter for the number of songs encountered\r\n",
    "songs_encountered = 0\r\n",
    "\r\n",
    "# Dictionary that will get one entry for each pack\r\n",
    "pack_data = {}\r\n",
    "\r\n",
    "# For every song pack\r\n",
    "for pack_name in song_packs:\r\n",
    "\r\n",
    "    # Empty dict that stores all relevant filepaths for a song inside a pack\r\n",
    "    # All unseen keys are assigned an empty list by default\r\n",
    "    song_data = defaultdict(lambda: [])\r\n",
    "\r\n",
    "    # Go through every file in the song pack\r\n",
    "    # (including files and subfiles)\r\n",
    "    for path, _, files in os.walk(join(base_path, pack_name)):\r\n",
    "\r\n",
    "        # For every file inside the base path\r\n",
    "        for file in files: \r\n",
    "\r\n",
    "            # Get the current file's parent folder (song folder)\r\n",
    "            # 1. The absolute path for the parent directory is extracted\r\n",
    "            # 2. 'normpath' strips off any trailing slashes\r\n",
    "            # 3. 'basename' returns the last part of the path\r\n",
    "            parent_name = basename(normpath(Path(path)))\r\n",
    "\r\n",
    "            # If the file has a \"pack_name\" as a parent the file is outside \r\n",
    "            # a song folder, it is ignored as a result.\r\n",
    "            if parent_name in song_packs:\r\n",
    "                print(f\"Found '{file}' outside of a song folder. Ignoring file.\")\r\n",
    "                continue\r\n",
    "\r\n",
    "            # The file extension is extracted\r\n",
    "            _, ext = splitext(file)\r\n",
    "\r\n",
    "            # File is deleted if it has an unwanted extension\r\n",
    "            if ext not in wanted_ext:\r\n",
    "                try:\r\n",
    "                    os.remove(join(path, file))\r\n",
    "                    files_deleted += 1\r\n",
    "                except Exception as e:\r\n",
    "                    raise Exception(e)\r\n",
    "\r\n",
    "            # All the paths that relate to a song are stored in a dict\r\n",
    "            # according to their name and the songpack they belong to\r\n",
    "            else:\r\n",
    "                song_data[parent_name].append(join(path, file))\r\n",
    "    \r\n",
    "    # The \"song_data\" is stored inside the \"pack_data\"\r\n",
    "    # (This is to prevent two packs having the same title for a\r\n",
    "    # song and risking overwriting the data for one song.)\r\n",
    "    pack_data[pack_name] = song_data\r\n",
    "\r\n",
    "    # We add the number of songs in the pack to \"songs_encountered\"\r\n",
    "    songs_encountered += len(list(song_data.keys()))\r\n",
    "\r\n",
    "# Printout after cleaning\r\n",
    "if files_deleted == 0:\r\n",
    "    print(f\"Dataset already clean. {songs_encountered} songs found. 0 files deleted.\")\r\n",
    "else:\r\n",
    "    print(f\"Dataset cleaned successfully. {songs_encountered} songs found. {files_deleted} files deleted.\")\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Found a total of 6 song packs.\n",
      "Found 'group.ini' outside of a song folder. Ignoring file.\n",
      "Dataset already clean. 230 songs found. 0 files deleted.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Check if Songs Have Both Audio and Note Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# Dict for the path of all audio files\r\n",
    "audio_paths = defaultdict(dict)\r\n",
    "\r\n",
    "# Number of songs with both audio and a stepfile\r\n",
    "complete_songs = 0\r\n",
    "\r\n",
    "# For every song in each pack\r\n",
    "for pack_name in pack_data.keys():\r\n",
    "    for song_name in pack_data[pack_name].keys():\r\n",
    "\r\n",
    "        # We get all the extensions found for a song\r\n",
    "        song_folder_exts = [splitext(path)[1] for path in pack_data[pack_name][song_name]]\r\n",
    "\r\n",
    "        # Check one or more audio extensions were found inside the song folder\r\n",
    "        audio_check = any([audio_ext in song_folder_exts for audio_ext in audio_exts])\r\n",
    "\r\n",
    "        # Check if the required stepfile extension was found\r\n",
    "        sm_check = stepfile_ext in song_folder_exts\r\n",
    "\r\n",
    "        # If the song doesnt pass both checks, the song gets deleted from the dict\r\n",
    "        if not(audio_check and sm_check):\r\n",
    "\r\n",
    "            del pack_data[pack_name][song_name]\r\n",
    "            print(f\"Song '{song_name}' of pack '{pack_name}' does not contain one of the required files for training. Removing song from dataset.\")\r\n",
    "\r\n",
    "        else:\r\n",
    "            # We extract the path for the songs audio file\r\n",
    "            # A path is extracted only if it has one of the required extensions\r\n",
    "            audio_path = [path for path in pack_data[pack_name][song_name] if splitext(path)[1] in audio_exts]\r\n",
    "\r\n",
    "            # The path is added to an \"audio files\" dict\r\n",
    "            audio_paths[pack_name][song_name] = audio_path\r\n",
    "\r\n",
    "            # Increase the number of complete songs by one\r\n",
    "            complete_songs += 1\r\n",
    "\r\n",
    "\r\n",
    "print(f\"Complete songs: {complete_songs} / {songs_encountered} (Contained both audio and a stepfile file).\")\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Complete songs: 230 / 230 (Contained both audio and a stepfile file).\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tag Parsing Stepfiles"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# Counter for songs successfully processed\r\n",
    "successfully_processed = 0\r\n",
    "\r\n",
    "# Dict to store the tag data for each song in the pack \r\n",
    "tag_data = defaultdict(dict)\r\n",
    "\r\n",
    "# For every pack and song in the dataset\r\n",
    "for pack_name in pack_data.keys():\r\n",
    "    for song_name in tqdm(pack_data[pack_name], desc=f\"{pack_name}\"): \r\n",
    "\r\n",
    "        try:\r\n",
    "            # For every path corresponding to the current song, we take\r\n",
    "            # the one that contains the extension that we need. Due to it being\r\n",
    "            # returned inside of a list, we get the first element.\r\n",
    "            stepfile_path = [path for path in pack_data[pack_name][song_name] if stepfile_ext in path][0]\r\n",
    "\r\n",
    "        # If an error occurs while getting the stepfile path,\r\n",
    "        # the program skips the current song\r\n",
    "        except Exception:\r\n",
    "            print(f\"No '{stepfile_ext}' file found for song '{song_name}' in song pack '{pack_name}'. Skipping song.\")\r\n",
    "            continue\r\n",
    "\r\n",
    "        # Step file content is extracted as text\r\n",
    "        with open(stepfile_path, 'r', encoding=\"utf-8\") as stepfile:\r\n",
    "            stepfile_txt = stepfile.read()\r\n",
    "            \r\n",
    "        # The text of each song is parsed and turned into a dict of tags\r\n",
    "        tag_data[pack_name][song_name] = stepfile_parser(stepfile_txt)\r\n",
    "\r\n",
    "        # Required tags\r\n",
    "        required_tags = ['offset', 'bpms', 'notes']\r\n",
    "\r\n",
    "        # Current song tags\r\n",
    "        current_tags = list(tag_data[pack_name][song_name].keys())\r\n",
    "\r\n",
    "        # Check if resulting dictionary keys contain all the required tags\r\n",
    "        if not all((item in current_tags) for item in required_tags):\r\n",
    "            raise Exception(f\"Song '{song_name}' of pack '{pack_name}' does not contain all of the required tags: 'offset', 'bpms' and 'notes'.\")\r\n",
    "\r\n",
    "        # Increase the number of files succesfully processed\r\n",
    "        else:\r\n",
    "            successfully_processed += 1\r\n",
    "\r\n",
    "\r\n",
    "# Tag data is saved to a pickle file\r\n",
    "with open('tag_data.pickle', 'wb') as handle:\r\n",
    "    pickle.dump(tag_data, handle, protocol=pickle.HIGHEST_PROTOCOL)\r\n",
    "    print(\"Tag data successfully saved.\")\r\n",
    "\r\n",
    "# Successful files\r\n",
    "print(f\"Number of succesfully processed songs: {successfully_processed} / {songs_encountered}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Fraxtil's Arrow Arrangements: 100%|██████████| 20/20 [00:17<00:00,  1.17it/s]\n",
      "Fraxtil's Beast Beats: 100%|██████████| 20/20 [00:17<00:00,  1.16it/s]\n",
      "In The Groove: 100%|██████████| 67/67 [00:47<00:00,  1.43it/s]\n",
      "In The Groove 2: 100%|██████████| 66/66 [00:36<00:00,  1.82it/s]\n",
      "KDA - ALL OUT: 100%|██████████| 7/7 [00:03<00:00,  2.32it/s]\n",
      "Tsunamix III: 100%|██████████| 50/50 [00:13<00:00,  3.72it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of succesfully processed songs: 230 / 230\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Alternative: Load Tag Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# Load the tag data (deserialize)\r\n",
    "with open('tag_data.pickle', 'rb') as file:\r\n",
    "    tag_data = pickle.load(file)\r\n",
    "    print(\"Tag data succesfully loaded.\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Tag data succesfully loaded.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Add Timestamps to all Charts"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# Dictionary to store the processed data\r\n",
    "measure_data = defaultdict(dict)\r\n",
    "\r\n",
    "# For every pack and song in the dataset\r\n",
    "for pack_name in tag_data.keys():\r\n",
    "    for song_name in tag_data[pack_name].keys(): \r\n",
    "\r\n",
    "        # We store the post-processed measures\r\n",
    "        measure_data[pack_name][song_name] = add_measure_timestamps(tag_data[pack_name][song_name])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Audio to Spectrogram\r\n",
    "\r\n",
    "Based off the following article: https://towardsdatascience.com/getting-to-know-the-mel-spectrogram-31bca3e2d9d0"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "source": [
    "# Dict for all spectrogram data (all songs)\r\n",
    "audio_data = defaultdict(dict)\r\n",
    "\r\n",
    "# For every pack and song in the dataset\r\n",
    "for pack_name in tag_data.keys():\r\n",
    "\r\n",
    "    # We use \"file=sys.stdout\" to make the output nicer\r\n",
    "    for song_name in tqdm(tag_data[pack_name].keys(), desc=f\"{pack_name}\", file=sys.stdout): \r\n",
    "\r\n",
    "        # ===================\r\n",
    "        # LOADING AUDIO\r\n",
    "        # ===================\r\n",
    "\r\n",
    "        # Extract the path to the song's audio\r\n",
    "        audio_path = audio_paths[pack_name][song_name][0]\r\n",
    "\r\n",
    "        # Audio gets loaded\r\n",
    "        raw_audio, sample_rate = librosa.load(audio_path)\r\n",
    "\r\n",
    "        # ===================\r\n",
    "        # SPECTROGRAM (STFT)\r\n",
    "        # ===================\r\n",
    "\r\n",
    "        # Hyperparameters for Librosas's STFT\r\n",
    "        # Both the window size and the stride are given in miliseconds.\r\n",
    "        # \"n_mels\" consists of the number of frequency bins that the user after applying the \"Mel Scale\".\r\n",
    "        # We use three different window sizes to capture different amounts of \"detail\" in the signal.\r\n",
    "        window_sizes = [23, 46, 93]            \r\n",
    "        stride = 10\r\n",
    "        n_mels = 80\r\n",
    "\r\n",
    "        # 3D tensor for the all the STFT results after using each window size\r\n",
    "        spectrogram_data = []\r\n",
    "\r\n",
    "        # Lowest number of columns found so far\r\n",
    "        lowest_num_col = np.Inf\r\n",
    "\r\n",
    "        # For every window size (in ms)\r\n",
    "        for window_size in window_sizes:\r\n",
    "\r\n",
    "            # Calculate parameters for Short Time Fourier Transform (STFT)\r\n",
    "            n_fft      = int(round(window_size * sample_rate / 1e3))\r\n",
    "            hop_length = int(round(     stride * sample_rate / 1e3))\r\n",
    "\r\n",
    "            # Spectrogram is generated\r\n",
    "            spectrogram = librosa.feature.melspectrogram(raw_audio, sr=sample_rate, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels)\r\n",
    "\r\n",
    "            # Spectrogram gets scaled into decibels\r\n",
    "            spectrogram_db = librosa.power_to_db(spectrogram, ref=np.max)\r\n",
    "\r\n",
    "            # Keep a record of the lowest number of columns found\r\n",
    "            if spectrogram_db.shape[1] <= lowest_num_col:\r\n",
    "                lowest_num_col = spectrogram_db.shape[1]\r\n",
    "\r\n",
    "            # Reduce size of array in case the current number of columns is higher than the\r\n",
    "            # lowest recorded number of columns.\r\n",
    "            else:\r\n",
    "                sys.stderr.flush()\r\n",
    "                print(\"Spectrogram with a higher dimensionality found. Slicing to match the remaining sequences.\")\r\n",
    "                spectrogram_db = spectrogram_db[:,0:lowest_num_col]\r\n",
    "\r\n",
    "            # New data is appended\r\n",
    "            spectrogram_data.append(spectrogram_db)\r\n",
    "\r\n",
    "        # ===================\r\n",
    "        # SAVING AUDIO DATA\r\n",
    "        # ===================\r\n",
    "\r\n",
    "        # Resize array to have the shape: (Time x N Mel x 3)\r\n",
    "        spectrogram_data = np.reshape(np.array(spectrogram_data), (-1, n_mels, 3))\r\n",
    "\r\n",
    "        # Equally spaced values between 0 and the number of frames in the STFT\r\n",
    "        k = np.linspace(0, spectrogram_data.shape[0], spectrogram_data.shape[0])\r\n",
    "\r\n",
    "        # Convert the spectrogram frames into seconds (timestamps)\r\n",
    "        # (Get in which second a frame occurs)\r\n",
    "        time_data = librosa.core.frames_to_time(k, sample_rate, hop_length)\r\n",
    "\r\n",
    "        # Add the current \"spectrogram matrix\" to \"audio_data\"\r\n",
    "        audio_data[pack_name][song_name] = {\"spectrogram\": spectrogram_data, \"time\": time_data}\r\n",
    "\r\n",
    "# The resulting audio data is stored in a pickle file\r\n",
    "with open('audio_data.pickle', 'wb') as handle:\r\n",
    "    pickle.dump(audio_data, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Fraxtil's Arrow Arrangements: 100%|██████████| 20/20 [03:43<00:00, 11.18s/it]\n",
      "Fraxtil's Beast Beats: 100%|██████████| 20/20 [03:30<00:00, 10.51s/it]\n",
      "In The Groove:  85%|████████▌ | 57/67 [09:19<01:43, 10.31s/it]Spectrogram with a higher dimensionality found. Slicing to match the remaining sequences.\n",
      "In The Groove: 100%|██████████| 67/67 [11:05<00:00,  9.93s/it]\n",
      "In The Groove 2:  15%|█▌        | 10/66 [01:28<08:06,  8.69s/it]Spectrogram with a higher dimensionality found. Slicing to match the remaining sequences.\n",
      "In The Groove 2:  39%|███▉      | 26/66 [03:45<05:39,  8.49s/it]Spectrogram with a higher dimensionality found. Slicing to match the remaining sequences.\n",
      "In The Groove 2: 100%|██████████| 66/66 [09:41<00:00,  8.81s/it]\n",
      "KDA - ALL OUT: 100%|██████████| 7/7 [01:11<00:00, 10.17s/it]\n",
      "Tsunamix III:  60%|██████    | 30/50 [05:06<03:36, 10.81s/it]Spectrogram with a higher dimensionality found. Slicing to match the remaining sequences.\n",
      "Tsunamix III:  66%|██████▌   | 33/50 [05:34<02:45,  9.71s/it]Spectrogram with a higher dimensionality found. Slicing to match the remaining sequences.\n",
      "Tsunamix III: 100%|██████████| 50/50 [08:23<00:00, 10.08s/it]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Alternative: Load Audio Data\r\n",
    "\r\n",
    "If the audio data generation was run before, the user can re-load all the previously processed assets."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# Load the audio data (deserialize)\r\n",
    "with open('audio_data.pickle', 'rb') as file:\r\n",
    "    audio_data = pickle.load(file)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataset Creation and Augmentation\r\n",
    "\r\n",
    "The spectrogram data is used to create the input of the neural network ($X$), while the step data is used to generate the output data ($y$). The data is doubled (augmented) by flipping horizontally all steps in 3 different patterns: Left-right, up-down, Left-right and up-down. This is a suitable \"augmentation\" as it does not destroy the underlying patterns in the songs, and increases the available data by four times. We do not create copies of the input data as this would result in a ridiculously large dataset. We use the output vector's \"pack and song\" list to index the spectrogram data later."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\r\n",
    "\r\n",
    "# ======================\r\n",
    "# STEP CLASS ENCODING\r\n",
    "# ======================\r\n",
    "\r\n",
    "# There are 9 different types of step. We encode for each one:\r\n",
    "# 0 - No Note\r\n",
    "# 1 - Normal Note\r\n",
    "# 2 - Hold Head\r\n",
    "# 3 - Hold/Roll Tail\r\n",
    "# 4 - Roll Head\r\n",
    "# M - Mine (or \"bad\" note)\r\n",
    "# K - Automatic keysound\r\n",
    "# L - Lift note\r\n",
    "# F - Fake note\r\n",
    "num_step_classes = 9\r\n",
    "\r\n",
    "# One hot encoder fit for detecting the 9 step classes\r\n",
    "OHEnc_step = OneHotEncoder(handle_unknown='ignore')\r\n",
    "OHEnc_step.fit(np.reshape(np.arange(0, num_step_classes), (-1,1)))\r\n",
    "\r\n",
    "# One hot encoder for the difficulty\r\n",
    "difficulties = np.array([\"Beginner\", \"Easy\", \"Medium\", \"Hard\", \"Challenge\", \"Edit\"])\r\n",
    "OHEnc_diff = OneHotEncoder(handle_unknown='ignore')\r\n",
    "OHEnc_diff.fit(np.reshape(difficulties, (-1,1)))\r\n",
    "\r\n",
    "# Empty list for output data\r\n",
    "y_data = []\r\n",
    "\r\n",
    "# Iterate once again over every pack and song\r\n",
    "# We use \"file=sys.stdout\" to make the output nicer\r\n",
    "for pack in tag_data.keys():\r\n",
    "    for song in tqdm(tag_data[pack].keys(), desc=f\"{pack}\", file=sys.stdout): \r\n",
    "\r\n",
    "        # For every difficulty that uses a single pad\r\n",
    "        for difficulty in measure_data[pack][song][\"dance-single\"].keys():\r\n",
    "\r\n",
    "            # =======================\r\n",
    "            # STANDARD MEASURE DATA\r\n",
    "            # =======================\r\n",
    "\r\n",
    "            # One hot encoding of the difficulty\r\n",
    "            difficulty_OHE = OHEnc_diff.transform(np.array([[difficulty]])).toarray()\r\n",
    "            \r\n",
    "            # Extract the current difficulty chart\r\n",
    "            chart = measure_data[pack][song][\"dance-single\"][difficulty]\r\n",
    "\r\n",
    "            # Timestamp values for timing chart steps\r\n",
    "            step_timestamps = chart[:,4]\r\n",
    "\r\n",
    "            # P(Step): Probability of a step occurring\r\n",
    "            P_step = 1*np.any(chart[:,0:4], axis=1, keepdims=True)\r\n",
    "\r\n",
    "            # BPM: BPM value during each step\r\n",
    "            BPM_step = np.reshape(chart[:,5], (-1, 1))\r\n",
    "\r\n",
    "            # Probability of step class ocurring for each step direction \r\n",
    "            # - P(Left)  : Multiclass probability of a left step occurring\r\n",
    "            # - P(Down)  : Multiclass probability of a down step occurring\r\n",
    "            # - P(Up)    : Multiclass probability of a up step occurring\r\n",
    "            # - P(Right) : Multiclass probability of a right step occurring\r\n",
    "            P_left  = OHEnc_step.transform(np.reshape(chart[:,0], (-1, 1))).toarray()\r\n",
    "            P_down  = OHEnc_step.transform(np.reshape(chart[:,1], (-1, 1))).toarray()\r\n",
    "            P_up    = OHEnc_step.transform(np.reshape(chart[:,2], (-1, 1))).toarray()\r\n",
    "            P_right = OHEnc_step.transform(np.reshape(chart[:,3], (-1, 1))).toarray()\r\n",
    "\r\n",
    "            # Concatenation of the previous parts to form the output of the \r\n",
    "            # neural network for a \"singles\" song chart.\r\n",
    "            output_nn = np.hstack((P_step, BPM_step, P_left, P_down, P_up, P_right))\r\n",
    "\r\n",
    "            # Output tuple: \r\n",
    "            # - Name of the pack and the song the chart belongs to (to index the spectrogram data)\r\n",
    "            # - Chart data with the shape of the output of the neural net\r\n",
    "            # - Difficulty one hot encoded (to condition the output)\r\n",
    "            # - The step timestamps for timing each step\r\n",
    "            y_data.append(([pack, song], output_nn, difficulty_OHE, step_timestamps))\r\n",
    "\r\n",
    "            # =======================\r\n",
    "            # DATA AUGMENTATION\r\n",
    "            # =======================\r\n",
    "\r\n",
    "            # To augment the available data, we mirror the chart as follows:\r\n",
    "            # - Mirroring the up and down steps\r\n",
    "            # - Mirroring the left and right steps\r\n",
    "            # - Mirroring both up and down, and left and right\r\n",
    "            output_mirror1 = np.hstack((P_step, BPM_step, P_left, P_up, P_down, P_right))\r\n",
    "            output_mirror2 = np.hstack((P_step, BPM_step, P_right, P_down, P_up, P_left))\r\n",
    "            output_mirror3 = np.hstack((P_step, BPM_step, P_right, P_up, P_down, P_left))\r\n",
    "\r\n",
    "            # We append the augmented data to the output list\r\n",
    "            y_data.append(([pack, song], output_mirror1, difficulty_OHE, step_timestamps))\r\n",
    "            y_data.append(([pack, song], output_mirror2, difficulty_OHE, step_timestamps))\r\n",
    "            y_data.append(([pack, song], output_mirror3, difficulty_OHE, step_timestamps))\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Fraxtil's Arrow Arrangements: 100%|██████████| 20/20 [00:00<00:00, 44.03it/s]\n",
      "Fraxtil's Beast Beats: 100%|██████████| 20/20 [00:00<00:00, 41.16it/s]\n",
      "In The Groove: 100%|██████████| 67/67 [00:01<00:00, 56.50it/s]\n",
      "In The Groove 2: 100%|██████████| 66/66 [00:01<00:00, 53.84it/s]\n",
      "KDA - ALL OUT: 100%|██████████| 7/7 [00:00<00:00, 59.27it/s]\n",
      "Tsunamix III: 100%|██████████| 50/50 [00:00<00:00, 54.11it/s]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prepare the Data (Reshape and Train/Test/Valid Splits)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "source": [
    "from sklearn.model_selection import train_test_split\r\n",
    "\r\n",
    "# Training and validation splits are created:\r\n",
    "# (Test split will be created later by processing new songs)\r\n",
    "y_train, y_valid = train_test_split(y_data, test_size=0.2, random_state=69)\r\n",
    "\r\n",
    "# NOTE 1: We only split \"y\" as this varible can be then used to index the spectrogram data.\r\n",
    "# NOTE 2: We use 20% of the total data to validate."
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Generator "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "source": [
    "pack = \"KDA - ALL OUT\"\r\n",
    "name = \"MORE\"\r\n",
    "\r\n",
    "print(audio_data[pack][name][\"spectrogram\"].shape)\r\n",
    "print(audio_data[pack][name][\"time\"].shape)\r\n",
    "\r\n",
    "print(audio_data[pack][name][\"time\"])\r\n",
    "print(measure_data[pack][name][\"dance-single\"][\"Hard\"][:,4])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(14567, 80, 3)\n",
      "(14567,)\n",
      "[0.00000000e+00 9.97732426e-03 1.99546485e-02 ... 1.45319683e+02\n",
      " 1.45329660e+02 1.45339683e+02]\n",
      "[  0.           0.42353521   0.84607042   1.26860563   1.69114085\n",
      "   2.11367606   2.53621127   2.95874648   3.38128169   3.8038169\n",
      "   4.22635211   4.64888732   5.07142254   5.49395775   5.91649296\n",
      "   6.33902817   6.76156338   7.18409859   7.6066338    8.02916901\n",
      "   8.45170423   8.87423944   9.29677465   9.71930986  10.14184507\n",
      "  10.35311268  10.56438028  10.77564789  10.98691549  11.1981831\n",
      "  11.4094507   11.62071831  11.83198592  12.04325352  12.25452113\n",
      "  12.46578873  12.67705634  12.88832394  13.09959155  13.31085915\n",
      "  13.52212676  13.73339437  13.94466197  14.15592958  14.36719718\n",
      "  14.57846479  14.78973239  15.001       15.21226761  15.42353521\n",
      "  15.63480282  15.84607042  16.05733803  16.26860563  16.47987324\n",
      "  16.69114085  16.90240845  17.11367606  17.32494366  17.53621127\n",
      "  17.74747887  17.95874648  18.17001408  18.38128169  18.5925493\n",
      "  18.8038169   19.01508451  19.22635211  19.43761972  19.64888732\n",
      "  19.86015493  20.07142254  20.28269014  20.38832394  20.49395775\n",
      "  20.59959155  20.70522535  20.81085915  20.91649296  21.02212676\n",
      "  21.12776056  21.23339437  21.33902817  21.44466197  21.55029577\n",
      "  21.65592958  21.76156338  21.86719718  21.97283099  22.18409859\n",
      "  22.3953662   22.6066338   22.81790141  23.02916901  23.24043662\n",
      "  23.45170423  23.66297183  23.8038169   23.94466197  24.08550704\n",
      "  24.22635211  24.36719718  24.50804225  24.64888732  24.78973239\n",
      "  24.93057746  25.07142254  25.21226761  25.35311268  25.49395775\n",
      "  25.63480282  25.77564789  25.91649296  26.05733803  26.1981831\n",
      "  26.33902817  26.47987324  26.62071831  26.76156338  26.90240845\n",
      "  27.04325352  27.18409859  27.32494366  27.46578873  27.6066338\n",
      "  27.74747887  27.88832394  28.02916901  28.17001408  28.31085915\n",
      "  28.45170423  28.5925493   28.73339437  28.87423944  29.01508451\n",
      "  29.15592958  29.29677465  29.43761972  29.57846479  29.71930986\n",
      "  29.86015493  30.001       30.14184507  30.28269014  30.42353521\n",
      "  30.56438028  30.70522535  30.84607042  30.98691549  31.12776056\n",
      "  31.26860563  31.4094507   31.55029577  31.69114085  31.83198592\n",
      "  31.97283099  32.11367606  32.25452113  32.3953662   32.53621127\n",
      "  32.67705634  32.81790141  32.95874648  33.09959155  33.24043662\n",
      "  33.38128169  33.52212676  33.66297183  33.8038169   33.94466197\n",
      "  34.08550704  34.22635211  34.36719718  34.50804225  34.64888732\n",
      "  34.78973239  34.93057746  35.07142254  35.21226761  35.35311268\n",
      "  35.49395775  35.63480282  35.77564789  35.91649296  36.05733803\n",
      "  36.1981831   36.33902817  36.47987324  36.62071831  36.76156338\n",
      "  36.90240845  37.04325352  37.18409859  37.32494366  37.46578873\n",
      "  37.6066338   37.74747887  37.88832394  38.02916901  38.17001408\n",
      "  38.31085915  38.45170423  38.5925493   38.73339437  38.87423944\n",
      "  39.01508451  39.15592958  39.29677465  39.43761972  39.57846479\n",
      "  39.71930986  39.86015493  40.001       40.14184507  40.28269014\n",
      "  40.42353521  40.56438028  40.70522535  40.84607042  40.98691549\n",
      "  41.12776056  41.26860563  41.4094507   41.55029577  41.69114085\n",
      "  41.83198592  41.97283099  42.11367606  42.25452113  42.3953662\n",
      "  42.53621127  42.67705634  42.81790141  42.95874648  43.09959155\n",
      "  43.24043662  43.38128169  43.52212676  43.66297183  43.8038169\n",
      "  43.94466197  44.36719718  44.78973239  45.21226761  45.63480282\n",
      "  45.84607042  46.05733803  46.26860563  46.47987324  46.69114085\n",
      "  46.90240845  47.11367606  47.32494366  47.53621127  47.74747887\n",
      "  47.95874648  48.17001408  48.38128169  48.5925493   48.8038169\n",
      "  49.01508451  49.22635211  49.43761972  49.64888732  49.86015493\n",
      "  50.07142254  50.28269014  50.49395775  50.70522535  51.12776056\n",
      "  51.55029577  51.97283099  52.3953662   52.81790141  53.24043662\n",
      "  53.66297183  54.08550704  54.29677465  54.50804225  54.71930986\n",
      "  54.93057746  55.14184507  55.35311268  55.56438028  55.77564789\n",
      "  55.98691549  56.1981831   56.4094507   56.62071831  57.04325352\n",
      "  57.25452113  57.36015493  57.46578873  57.67705634  57.88832394\n",
      "  58.09959155  58.31085915  58.52212676  58.73339437  58.94466197\n",
      "  59.15592958  59.36719718  59.57846479  59.78973239  60.001\n",
      "  60.21226761  60.42353521  60.63480282  60.84607042  61.05733803\n",
      "  61.26860563  61.47987324  61.69114085  61.90240845  62.11367606\n",
      "  62.32494366  62.53621127  62.74747887  62.95874648  63.17001408\n",
      "  63.38128169  63.5925493   63.8038169   64.01508451  64.22635211\n",
      "  64.43761972  64.64888732  64.86015493  65.07142254  65.28269014\n",
      "  65.49395775  65.70522535  65.91649296  66.12776056  66.33902817\n",
      "  66.55029577  66.76156338  66.97283099  67.18409859  67.3953662\n",
      "  67.6066338   67.81790141  68.02916901  68.24043662  68.45170423\n",
      "  68.66297183  68.87423944  69.08550704  69.29677465  69.50804225\n",
      "  69.71930986  69.93057746  70.14184507  70.35311268  70.56438028\n",
      "  70.77564789  70.98691549  71.1981831   71.4094507   71.62071831\n",
      "  71.83198592  72.04325352  72.25452113  72.46578873  72.67705634\n",
      "  72.88832394  73.09959155  73.31085915  73.52212676  73.73339437\n",
      "  73.94466197  74.15592958  74.36719718  74.57846479  74.78973239\n",
      "  75.001       75.21226761  75.42353521  75.63480282  75.84607042\n",
      "  76.05733803  76.26860563  76.47987324  76.69114085  76.90240845\n",
      "  77.11367606  77.32494366  77.53621127  77.74747887  77.95874648\n",
      "  78.17001408  78.38128169  78.5925493   78.8038169   79.01508451\n",
      "  79.22635211  79.43761972  79.64888732  79.86015493  80.07142254\n",
      "  80.28269014  80.49395775  80.70522535  80.91649296  81.12776056\n",
      "  81.33902817  81.55029577  81.76156338  81.97283099  82.18409859\n",
      "  82.3953662   82.6066338   82.81790141  83.02916901  83.24043662\n",
      "  83.45170423  83.66297183  83.87423944  84.08550704  84.29677465\n",
      "  84.50804225  84.93057746  85.35311268  85.77564789  86.1981831\n",
      "  86.62071831  87.04325352  87.46578873  87.88832394  88.09959155\n",
      "  88.31085915  88.52212676  88.73339437  88.94466197  89.15592958\n",
      "  89.36719718  89.57846479  90.001       90.42353521  90.84607042\n",
      "  91.26860563  91.47987324  91.69114085  91.90240845  92.11367606\n",
      "  92.32494366  92.53621127  92.74747887  92.95874648  93.38128169\n",
      "  93.8038169   94.22635211  94.64888732  94.86015493  95.07142254\n",
      "  95.28269014  95.49395775  95.70522535  95.91649296  96.12776056\n",
      "  96.33902817  96.76156338  97.18409859  97.6066338   98.02916901\n",
      "  98.24043662  98.45170423  98.66297183  98.87423944  99.08550704\n",
      "  99.29677465  99.50804225  99.71930986  99.93057746 100.14184507\n",
      " 100.35311268 100.56438028 100.77564789 100.98691549 101.1981831\n",
      " 101.4094507  101.62071831 101.83198592 102.04325352 102.25452113\n",
      " 102.46578873 102.67705634 102.88832394 103.09959155 103.31085915\n",
      " 103.52212676 103.73339437 103.94466197 104.15592958 104.36719718\n",
      " 104.57846479 104.78973239 105.001      105.21226761 105.42353521\n",
      " 105.63480282 105.84607042 106.05733803 106.26860563 106.47987324\n",
      " 106.69114085 106.90240845 107.11367606 107.32494366 107.53621127\n",
      " 107.74747887 107.95874648 108.17001408 108.38128169 108.5925493\n",
      " 108.8038169  109.01508451 109.22635211 109.43761972 109.64888732\n",
      " 109.86015493 110.07142254 110.28269014 110.49395775 110.70522535\n",
      " 110.91649296 111.12776056 111.33902817 111.55029577 111.76156338\n",
      " 111.97283099 112.18409859 112.3953662  112.81790141 113.02916901\n",
      " 113.13480282 113.24043662 113.45170423 113.66297183 113.87423944\n",
      " 114.08550704 114.29677465 114.50804225 114.71930986 114.93057746\n",
      " 115.14184507 115.35311268 115.56438028 115.77564789 115.98691549\n",
      " 116.1981831  116.4094507  116.62071831 116.83198592 117.04325352\n",
      " 117.25452113 117.46578873 117.67705634 117.88832394 118.09959155\n",
      " 118.31085915 118.52212676 118.73339437 118.94466197 119.15592958\n",
      " 119.36719718 119.57846479 119.78973239 120.001      120.21226761\n",
      " 120.42353521 120.63480282 120.84607042 121.05733803 121.26860563\n",
      " 121.47987324 121.69114085 121.90240845 122.11367606 122.32494366\n",
      " 122.53621127 122.74747887 122.95874648 123.17001408 123.38128169\n",
      " 123.5925493  123.8038169  124.01508451 124.22635211 124.43761972\n",
      " 124.64888732 124.86015493 125.07142254 125.28269014 125.49395775\n",
      " 125.70522535 125.91649296 126.12776056 126.33902817 126.55029577\n",
      " 126.76156338 126.97283099 127.18409859 127.3953662  127.6066338\n",
      " 127.81790141 128.02916901 128.24043662 128.45170423 128.66297183\n",
      " 128.87423944 129.08550704 129.29677465 129.50804225 129.71930986\n",
      " 129.93057746 130.14184507 130.35311268 130.56438028 130.77564789\n",
      " 130.98691549 131.1981831  131.4094507  131.62071831 131.83198592\n",
      " 132.04325352 132.25452113 132.46578873 132.67705634 132.88832394\n",
      " 133.09959155 133.31085915 133.52212676 133.73339437 133.94466197\n",
      " 134.15592958 134.36719718 134.57846479 134.78973239 135.001\n",
      " 135.21226761 135.42353521 135.63480282 135.84607042 136.05733803\n",
      " 136.26860563 136.47987324 136.69114085 136.90240845 137.11367606\n",
      " 137.32494366 137.53621127 137.74747887 137.95874648 138.17001408\n",
      " 138.38128169 138.5925493  138.8038169  139.01508451 139.22635211\n",
      " 139.43761972 139.64888732 139.86015493 140.07142254]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "class TimeSliceGenerator(keras.utils.Sequence):\r\n",
    "\r\n",
    "    def __init__(self, X_data, y_data, batch_size=32, shuffle=True):\r\n",
    "        pass\r\n",
    "\r\n",
    "    def on_epoch_end(self):\r\n",
    "        pass\r\n",
    "\r\n",
    "    def __getitem__(self, index):\r\n",
    "        pass\r\n",
    "\r\n",
    "    def __len__(self):\r\n",
    "        return self.n // self.batch_size"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Weights and Biases (WandB) Setup"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# RemoveS excesive notifications from WandB during training\r\n",
    "logger = logging.getLogger(\"wandb\")\r\n",
    "logger.setLevel(logging.ERROR)\r\n",
    "\r\n",
    "# Make the WandB stdout shut up\r\n",
    "os.environ[\"WANDB_SILENT\"] = \"True\"\r\n",
    "\r\n",
    "# Name of the current notebook according to WandB\r\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"Dance Dance Convolution - Revisited.ipynb\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Keras Model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "source": [
    "# ==================\r\n",
    "# SETTINGS\r\n",
    "# ==================\r\n",
    "\r\n",
    "# WandB: 0. Weights and Biases login (only if first time use)\r\n",
    "# WandB: 1. New run declaration with all the parameters to track.\r\n",
    "run = wandb.init(project=\"DanceDanceConvolutionX\", entity=\"sanoli\",\r\n",
    "                 config={\r\n",
    "                    \"learning_rate\": 0.001, \r\n",
    "                    \"epochs\": 10, \r\n",
    "                    \"batch_size\": 32,  \r\n",
    "                    \"loss_function\": {\r\n",
    "                       \"OUT_Stp\": \"binary_crossentropy\",\r\n",
    "                       \"OUT_bpm\": \"mse\",\r\n",
    "                       \"OUT_PLeft\" : \"categorical_crossentropy\",\r\n",
    "                       \"OUT_PRight\": \"categorical_crossentropy\",\r\n",
    "                       \"OUT_PUp\"   : \"categorical_crossentropy\",\r\n",
    "                       \"OUT_PDown\" : \"categorical_crossentropy\"\r\n",
    "                    },\r\n",
    "                    \"loss_weights\": {\r\n",
    "                       \"OUT_Stp\": 1,\r\n",
    "                       \"OUT_bpm\": 1,\r\n",
    "                       \"OUT_PLeft\" : 1,\r\n",
    "                       \"OUT_PRight\": 1,\r\n",
    "                       \"OUT_PUp\"   : 1,\r\n",
    "                       \"OUT_PDown\" : 1\r\n",
    "                    },\r\n",
    "                    \"architecture\": \"CNN + BiLSTM + MLP\",  \r\n",
    "                    \"dataset\": \"Fraxtil, KDA, ITG1, ITG2\"\r\n",
    "                 })\r\n",
    "\r\n",
    "\r\n",
    "# Network Weight initialization\r\n",
    "# It makes use of the 'glorot' initializer, also known as Xavier's initializer\r\n",
    "initializer = keras.initializers.GlorotNormal()\r\n",
    "\r\n",
    "# Number of time slices to feed the network at the same time.\r\n",
    "# (Kinda like \"time batches\")\r\n",
    "\r\n",
    "# Hyperparameters: \r\n",
    "# - batch_size: Number of samples fed to the NN at the same time\r\n",
    "# - time_batch_size: Number of time slices fed to the network at the same time\r\n",
    "batch_size = 32\r\n",
    "time_batch_size = 5\r\n",
    "\r\n",
    "# Difficulties\r\n",
    "# - The OHE is casted as a float\r\n",
    "# - We repeat the pattern as many times down as there are \"time slices\"\r\n",
    "# - We add an additional 3rd dimension to facilitate the concatenation\r\n",
    "difficulty = np.array([[0,0,0,0,1]]).astype(float)\r\n",
    "difficulty = np.tile(difficulty, (time_batch_size,1))\r\n",
    "difficulty = np.reshape(difficulty, (-1,time_batch_size, 5))\r\n",
    "\r\n",
    "# ==================\r\n",
    "# LAYERS\r\n",
    "# ==================\r\n",
    "\r\n",
    "# BLOCK 1: CONVOLUTION\r\n",
    "# - TimeDistributed is used to add a time dimension to the layers\r\n",
    "# - Rule of thumb: Dimension of convolution and max pooling has to match\r\n",
    "# - The charts difficulty (OHE) is concatenated with the flattened tensor\r\n",
    "IN  = Input(shape=(time_batch_size, 15, 80, 3), name=\"Input\")\r\n",
    "CL1 = TimeDistributed(Conv2D(filters=10, kernel_size=(7,3), strides=1, activation=\"relu\", padding=\"same\"), name=\"Conv2D-1\")(IN)\r\n",
    "MP1 = TimeDistributed(MaxPool2D(pool_size=3, strides=1), name=\"MaxPool2D-1\")(CL1)\r\n",
    "CL2 = TimeDistributed(Conv2D(filters=20, kernel_size=(3,3), strides=1, activation=\"relu\", padding=\"same\"), name=\"Conv2D-2\")(MP1)\r\n",
    "MP2 = TimeDistributed(MaxPool2D(pool_size=3, strides=3), name=\"MaxPool2D-2\")(CL2)\r\n",
    "F1  = TimeDistributed(Flatten(), name=\"Flatten\")(MP2)\r\n",
    "CC1 = Concatenate(axis=2, name=\"Concat-Diff\")([F1, difficulty])\r\n",
    "\r\n",
    "# BLOCK 2: FULLY CONNECTED\r\n",
    "D1  = Dense(units=256, activation=\"relu\", kernel_initializer=initializer, name=\"Dense-1\")(CC1)\r\n",
    "D2  = Dense(units=128, activation=\"relu\", kernel_initializer=initializer, name=\"Dense-2\")(D1)\r\n",
    "\r\n",
    "# BLOCK 3: RECURRENT NET\r\n",
    "# We add dropout layers in between each BiLSTM\r\n",
    "BL1 = Bidirectional(LSTM(200, return_sequences=True), name=\"BiLSTM-1\")(D2)\r\n",
    "DR1 = Dropout(0.2, name=\"Dropout-1\")(BL1)\r\n",
    "BL2 = Bidirectional(LSTM(200, return_sequences=True), name=\"BiLSTM-2\")(DR1)\r\n",
    "DR2 = Dropout(0.2, name=\"Dropout-2\")(BL2)\r\n",
    "BL3 = Bidirectional(LSTM(200, return_sequences=True), name=\"BiLSTM-3\")(DR2)\r\n",
    "\r\n",
    "# BLOCK 4: FULLY CONNECTED 2 (ELECTRIC BOOGALOO)\r\n",
    "D3  = Dense(units=128, activation=\"relu\", kernel_initializer=initializer, name=\"Dense-3\")(BL3)\r\n",
    "D4  = Dense(units= 64, activation=\"relu\", kernel_initializer=initializer, name=\"Dense-4\")(D3)\r\n",
    "\r\n",
    "# BLOCK 5: SEPARATE OUTPUTS\r\n",
    "# y[0]     = Probability of step. Sigmoid\r\n",
    "# y[1]     = BPM value for current step. ReLu\r\n",
    "# y[2:10]  = Probability distribution of \"Left\" step (One for each note type. 9 in total). Softmax\r\n",
    "# y[11:19] = Probability distribution of \"Down\" step (One for each note type. 9 in total). Softmax\r\n",
    "# y[20:28] = Probability distribution of \"Up\" step (One for each note type. 9 in total). Softmax\r\n",
    "# y[29:37] = Probability distribution of \"Right\" step (One for each note type. 9 in total). Softmax\r\n",
    "OUT_Stp    = Dense(units=1, activation=\"sigmoid\", name=\"Prob-Step\")(D4)\r\n",
    "OUT_bpm    = Dense(units=1, activation=\"relu\", name=\"BPM\")(D4)\r\n",
    "OUT_PLeft  = Dense(units=9, activation=\"softmax\", name=\"Prob-Left\")(D4)\r\n",
    "OUT_PRight = Dense(units=9, activation=\"softmax\", name=\"Prob-Right\")(D4)\r\n",
    "OUT_PUp    = Dense(units=9, activation=\"softmax\", name=\"Prob-Up\")(D4)\r\n",
    "OUT_PDown  = Dense(units=9, activation=\"softmax\", name=\"Prob-Down\")(D4)\r\n",
    "\r\n",
    "# BLOCK 6: FINAL CONCATENATION\r\n",
    "OUT = Concatenate(name=\"Concat-Outs\")([OUT_Stp, OUT_bpm, OUT_PLeft, OUT_PRight, OUT_PUp, OUT_PDown])\r\n",
    "\r\n",
    "# ==================\r\n",
    "# MODEL CREATION\r\n",
    "# ==================\r\n",
    "\r\n",
    "# A model is created with all the previous parts\r\n",
    "model = Model(inputs=IN, outputs=OUT, name=\"DDCX\")\r\n",
    "\r\n",
    "# WandB: 2. Stores the inputs and hyperparameters of the model\r\n",
    "config = wandb.config\r\n",
    "\r\n",
    "# The optimizer is selected\r\n",
    "optimizer = adam_v2.Adam(learning_rate=config.learning_rate)\r\n",
    "\r\n",
    "# The model is compiled\r\n",
    "model.compile(optimizer=optimizer, metrics=[\"accuracy\"], loss=config.loss_function, loss_weights=config.loss_weights)\r\n",
    "\r\n",
    "# The keras summary is stored as a WandB log\r\n",
    "wandb.log({\"summary\": model.summary()})"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"DDCX\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Input (InputLayer)              [(None, 5, 15, 80, 3 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Conv2D-1 (TimeDistributed)      (None, 5, 15, 80, 10 640         Input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "MaxPool2D-1 (TimeDistributed)   (None, 5, 13, 78, 10 0           Conv2D-1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "Conv2D-2 (TimeDistributed)      (None, 5, 13, 78, 20 1820        MaxPool2D-1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "MaxPool2D-2 (TimeDistributed)   (None, 5, 4, 26, 20) 0           Conv2D-2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "Flatten (TimeDistributed)       (None, 5, 2080)      0           MaxPool2D-2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Concat-Diff (Concatenate)       (1, 5, 2085)         0           Flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "Dense-1 (Dense)                 (1, 5, 256)          534016      Concat-Diff[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Dense-2 (Dense)                 (1, 5, 128)          32896       Dense-1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "BiLSTM-1 (Bidirectional)        (1, 5, 400)          526400      Dense-2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "Dropout-1 (Dropout)             (1, 5, 400)          0           BiLSTM-1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "BiLSTM-2 (Bidirectional)        (1, 5, 400)          961600      Dropout-1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Dropout-2 (Dropout)             (1, 5, 400)          0           BiLSTM-2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "BiLSTM-3 (Bidirectional)        (1, 5, 400)          961600      Dropout-2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Dense-3 (Dense)                 (1, 5, 128)          51328       BiLSTM-3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "Dense-4 (Dense)                 (1, 5, 64)           8256        Dense-3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "Prob-Step (Dense)               (1, 5, 1)            65          Dense-4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "BPM (Dense)                     (1, 5, 1)            65          Dense-4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "Prob-Left (Dense)               (1, 5, 9)            585         Dense-4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "Prob-Right (Dense)              (1, 5, 9)            585         Dense-4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "Prob-Up (Dense)                 (1, 5, 9)            585         Dense-4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "Prob-Down (Dense)               (1, 5, 9)            585         Dense-4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "Concat-Outs (Concatenate)       (1, 5, 38)           0           Prob-Step[0][0]                  \n",
      "                                                                 BPM[0][0]                        \n",
      "                                                                 Prob-Left[0][0]                  \n",
      "                                                                 Prob-Right[0][0]                 \n",
      "                                                                 Prob-Up[0][0]                    \n",
      "                                                                 Prob-Down[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 3,081,026\n",
      "Trainable params: 3,081,026\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model Training"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Se entrena el modelo \r\n",
    "model.fit(X_train, y_train, epochs=config.epochs, batch_size=config.batch_size, validation_data=(X_valid, y_valid), \r\n",
    "       callbacks=[\r\n",
    "            # Se envían datos a weights and biases\r\n",
    "            WandbCallback(\r\n",
    "                data_type=\"image\",                      # Se generan imágenes en el reporte\r\n",
    "                monitor=\"accuracy\",                     # Monitorea el accuracy como métrica\r\n",
    "                mode=\"max\",                             # Trackea aumentos en accuracy\r\n",
    "                save_model=True,                        # Guardar modelo cuando se alcanza un nuevo máximo en accuracy\r\n",
    "                validation_data=(X_valid, y_valid),     # WandB hace predicciones a medio proceso y las despliega en el dashboard\r\n",
    "            ),          \r\n",
    "       ])\r\n",
    "\r\n",
    "# Se mide la precisión con set pruebas\r\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\r\n",
    "print(f\"Test Error Rate: {round((1-accuracy)*100, 2)}\")\r\n",
    "\r\n",
    "# Se loguean los resultados en WandB\r\n",
    "wandb.log({\"Test Error Rate\" : round((1-accuracy)*100, 2)})\r\n",
    "run.join()\r\n",
    "\r\n",
    "# Se finaliza el \"run\" de WandB\r\n",
    "run.finish()"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.2",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.2 64-bit"
  },
  "interpreter": {
   "hash": "658dc12c475a3a8caebf03b24f414cffa2901ebd330ffd26b9c22f028a90850c"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}