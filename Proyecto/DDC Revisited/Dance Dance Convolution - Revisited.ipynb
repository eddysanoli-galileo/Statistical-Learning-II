{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Dance Dance Convolution - Revisited\r\n",
    "\r\n",
    "Revision of the original \"Dance Dance Convolution\" paper, that incorporates newer machine learning and AI techniques to \"hopefully\" improve the original model results."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Imports"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import os\r\n",
    "import sys\r\n",
    "import pickle\r\n",
    "import logging\r\n",
    "from tqdm import tqdm\r\n",
    "from pathlib import Path\r\n",
    "from collections import defaultdict\r\n",
    "from os.path import isfile, join, splitext, basename, normpath, exists\r\n",
    "\r\n",
    "import numpy as np\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import librosa\r\n",
    "import librosa.display\r\n",
    "\r\n",
    "# Weights and Biases (WandB)\r\n",
    "import wandb \r\n",
    "from wandb.keras import WandbCallback\r\n",
    "\r\n",
    "# Custom Functions\r\n",
    "from sm_parsing import stepfile_parser\r\n",
    "from post_process import add_measure_timestamps, log_spectrogram"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loading and Cleaning Dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# Path to dataset\r\n",
    "base_path = \"./dataset\"\r\n",
    "\r\n",
    "# Extension of the required stepfile\r\n",
    "stepfile_ext = \".sm\"\r\n",
    "\r\n",
    "# Allowed audio and stepfile extensions\r\n",
    "audio_exts = [\".ogg\", \".mp3\", \".wav\"]\r\n",
    "steps_exts = [\".sm\", \".ssc\"]\r\n",
    "\r\n",
    "# Song packs inside dataset\r\n",
    "# (Ignores files that are not directories)\r\n",
    "song_packs = [f for f in os.listdir(base_path) if not isfile(join(base_path, f))]\r\n",
    "\r\n",
    "print(f\"Found a total of {len(song_packs)} song packs.\")\r\n",
    "\r\n",
    "# =======================================\r\n",
    "# SONG PACK CLEANING AND DATA EXTRACTION\r\n",
    "# =======================================\r\n",
    "\r\n",
    "# Files that dont add anything to training are deleted (videos, images, txts, etc.)\r\n",
    "# Wanted file extensions\r\n",
    "wanted_ext = audio_exts + steps_exts\r\n",
    "\r\n",
    "# Counter for the number of files deleted.\r\n",
    "files_deleted = 0\r\n",
    "\r\n",
    "# Counter for the number of songs encountered\r\n",
    "songs_encountered = 0\r\n",
    "\r\n",
    "# Dictionary that will get one entry for each pack\r\n",
    "pack_data = {}\r\n",
    "\r\n",
    "# For every song pack\r\n",
    "for pack_name in song_packs:\r\n",
    "\r\n",
    "    # Empty dict that stores all relevant filepaths for a song inside a pack\r\n",
    "    # All unseen keys are assigned an empty list by default\r\n",
    "    song_data = defaultdict(lambda: [])\r\n",
    "\r\n",
    "    # Go through every file in the song pack\r\n",
    "    # (including files and subfiles)\r\n",
    "    for path, _, files in os.walk(join(base_path, pack_name)):\r\n",
    "\r\n",
    "        # For every file inside the base path\r\n",
    "        for file in files: \r\n",
    "\r\n",
    "            # Get the current file's parent folder (song folder)\r\n",
    "            # 1. The absolute path for the parent directory is extracted\r\n",
    "            # 2. 'normpath' strips off any trailing slashes\r\n",
    "            # 3. 'basename' returns the last part of the path\r\n",
    "            parent_name = basename(normpath(Path(path)))\r\n",
    "\r\n",
    "            # If the file has a \"pack_name\" as a parent the file is outside \r\n",
    "            # a song folder, it is ignored as a result.\r\n",
    "            if parent_name in song_packs:\r\n",
    "                print(f\"Found '{file}' outside of a song folder. Ignoring file.\")\r\n",
    "                continue\r\n",
    "\r\n",
    "            # The file extension is extracted\r\n",
    "            _, ext = splitext(file)\r\n",
    "\r\n",
    "            # File is deleted if it has an unwanted extension\r\n",
    "            if ext not in wanted_ext:\r\n",
    "                try:\r\n",
    "                    os.remove(join(path, file))\r\n",
    "                    files_deleted += 1\r\n",
    "                except Exception as e:\r\n",
    "                    raise Exception(e)\r\n",
    "\r\n",
    "            # All the paths that relate to a song are stored in a dict\r\n",
    "            # according to their name and the songpack they belong to\r\n",
    "            else:\r\n",
    "                song_data[parent_name].append(join(path, file))\r\n",
    "    \r\n",
    "    # The \"song_data\" is stored inside the \"pack_data\"\r\n",
    "    # (This is to prevent two packs having the same title for a\r\n",
    "    # song and risking overwriting the data for one song.)\r\n",
    "    pack_data[pack_name] = song_data\r\n",
    "\r\n",
    "    # We add the number of songs in the pack to \"songs_encountered\"\r\n",
    "    songs_encountered += len(list(song_data.keys()))\r\n",
    "\r\n",
    "# Printout after cleaning\r\n",
    "if files_deleted == 0:\r\n",
    "    print(f\"Dataset already clean. {songs_encountered} songs found. 0 files deleted.\")\r\n",
    "else:\r\n",
    "    print(f\"Dataset cleaned successfully. {songs_encountered} songs found. {files_deleted} files deleted.\")\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Found a total of 6 song packs.\n",
      "Found 'group.ini' outside of a song folder. Ignoring file.\n",
      "Dataset already clean. 230 songs found. 0 files deleted.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Check if Songs Have Both Audio and Note Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# Dict for the path of all audio files\r\n",
    "audio_paths = defaultdict(dict)\r\n",
    "\r\n",
    "# For every song in each pack\r\n",
    "for pack_name in pack_data.keys():\r\n",
    "    for song_name in pack_data[pack_name].keys():\r\n",
    "\r\n",
    "        # We get all the extensions found for a song\r\n",
    "        song_folder_exts = [splitext(path)[1] for path in pack_data[pack_name][song_name]]\r\n",
    "\r\n",
    "        # Check one or more audio extensions were found inside the song folder\r\n",
    "        audio_check = any([audio_ext in song_folder_exts for audio_ext in audio_exts])\r\n",
    "\r\n",
    "        # Check if the required stepfile extension was found\r\n",
    "        sm_check = stepfile_ext in song_folder_exts\r\n",
    "\r\n",
    "        # If the song doesnt pass both checks, the song gets deleted from the dict\r\n",
    "        if not(audio_check and sm_check):\r\n",
    "\r\n",
    "            del pack_data[pack_name][song_name]\r\n",
    "            print(f\"Song '{song_name}' of pack '{pack_name}' does not contain one of the required files for training. Removing song from dataset.\")\r\n",
    "\r\n",
    "        else:\r\n",
    "            # We extract the path for the songs audio file\r\n",
    "            # A path is extracted only if it has one of the required extensions\r\n",
    "            audio_path = [path for path in pack_data[pack_name][song_name] if splitext(path)[1] in audio_exts]\r\n",
    "\r\n",
    "            # The path is added to an \"audio files\" dict\r\n",
    "            audio_paths[pack_name][song_name] = audio_path\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tag Parsing Stepfiles"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# Counter for songs successfully processed\r\n",
    "successfully_processed = 0\r\n",
    "\r\n",
    "# Dict to store the tag data for each song in the pack \r\n",
    "dataset_tags = defaultdict(dict)\r\n",
    "\r\n",
    "# For every pack and song in the dataset\r\n",
    "for pack_name in pack_data.keys():\r\n",
    "    for song_name in tqdm(pack_data[pack_name], desc=f\"{pack_name}\"): \r\n",
    "\r\n",
    "        try:\r\n",
    "            # For every path corresponding to the current song, we take\r\n",
    "            # the one that contains the extension that we need. Due to it being\r\n",
    "            # returned inside of a list, we get the first element.\r\n",
    "            stepfile_path = [path for path in pack_data[pack_name][song_name] if stepfile_ext in path][0]\r\n",
    "\r\n",
    "        # If an error occurs while getting the stepfile path,\r\n",
    "        # the program skips the current song\r\n",
    "        except Exception:\r\n",
    "            print(f\"No '{stepfile_ext}' file found for song '{song_name}' in song pack '{pack_name}'. Skipping song.\")\r\n",
    "            continue\r\n",
    "\r\n",
    "        # Step file content is extracted as text\r\n",
    "        with open(stepfile_path, 'r', encoding=\"utf-8\") as stepfile:\r\n",
    "            stepfile_txt = stepfile.read()\r\n",
    "            \r\n",
    "        # The text of each song is parsed and turned into a dict of tags\r\n",
    "        dataset_tags[pack_name][song_name] = stepfile_parser(stepfile_txt)\r\n",
    "\r\n",
    "        # Required tags\r\n",
    "        required_tags = ['offset', 'bpms', 'notes']\r\n",
    "\r\n",
    "        # Current song tags\r\n",
    "        current_tags = list(dataset_tags[pack_name][song_name].keys())\r\n",
    "\r\n",
    "        # Check if resulting dictionary keys contain all the required tags\r\n",
    "        if not all((item in current_tags) for item in required_tags):\r\n",
    "            raise Exception(f\"Song '{song_name}' of pack '{pack_name}' does not contain all of the required tags: 'offset', 'bpms' and 'notes'.\")\r\n",
    "\r\n",
    "        # Increase the number of files succesfully processed\r\n",
    "        else:\r\n",
    "            successfully_processed += 1\r\n",
    "\r\n",
    "# Successful files\r\n",
    "print(f\"Number of succesfully processed songs: {successfully_processed} / {songs_encountered}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Fraxtil's Arrow Arrangements: 100%|██████████| 20/20 [00:14<00:00,  1.35it/s]\n",
      "Fraxtil's Beast Beats: 100%|██████████| 20/20 [00:15<00:00,  1.33it/s]\n",
      "In The Groove: 100%|██████████| 67/67 [00:32<00:00,  2.04it/s]\n",
      "In The Groove 2: 100%|██████████| 66/66 [00:29<00:00,  2.26it/s]\n",
      "KDA - ALL OUT: 100%|██████████| 7/7 [00:02<00:00,  2.51it/s]\n",
      "Tsunamix III: 100%|██████████| 50/50 [00:12<00:00,  4.10it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of succesfully processed songs: 230 / 230\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Add Timestamps to all Charts"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# Dictionary to store the processed data\r\n",
    "measure_data = defaultdict(dict)\r\n",
    "\r\n",
    "# For every pack and song in the dataset\r\n",
    "for pack_name in dataset_tags.keys():\r\n",
    "    for song_name in dataset_tags[pack_name].keys(): \r\n",
    "\r\n",
    "        # We store the post-processed measures\r\n",
    "        measure_data[pack_name][song_name] = add_measure_timestamps(dataset_tags[pack_name][song_name])\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Audio to Spectrogram\r\n",
    "\r\n",
    "Based off the following article: https://towardsdatascience.com/getting-to-know-the-mel-spectrogram-31bca3e2d9d0"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "source": [
    "# Dict for all spectrogram data (all songs)\r\n",
    "audio_data = defaultdict(dict)\r\n",
    "\r\n",
    "# For every pack and song in the dataset\r\n",
    "for pack_name in dataset_tags.keys():\r\n",
    "\r\n",
    "    # We use \"file=sys.stdout\" to make the output nicer\r\n",
    "    for song_name in tqdm(dataset_tags[pack_name].keys(), desc=f\"{pack_name}\", file=sys.stdout): \r\n",
    "\r\n",
    "        # ===================\r\n",
    "        # LOADING AUDIO\r\n",
    "        # ===================\r\n",
    "\r\n",
    "        # Extract the path to the song's audio\r\n",
    "        audio_path = audio_paths[pack_name][song_name][0]\r\n",
    "\r\n",
    "        # Audio gets loaded\r\n",
    "        raw_audio, sample_rate = librosa.load(audio_path)\r\n",
    "\r\n",
    "        # ===================\r\n",
    "        # SPECTROGRAM (STFT)\r\n",
    "        # ===================\r\n",
    "\r\n",
    "        # Hyperparameters for Librosas's STFT\r\n",
    "        # Both the window size and the stride are given in miliseconds.\r\n",
    "        # \"n_mels\" consists of the number of frequency bins that the user after applying the \"Mel Scale\".\r\n",
    "        # We use three different window sizes to capture different amounts of \"detail\" in the signal.\r\n",
    "        window_sizes = [23, 46, 93]            \r\n",
    "        stride = 10\r\n",
    "        n_mels = 80\r\n",
    "\r\n",
    "        # 3D tensor for the all the STFT results after using each window size\r\n",
    "        spectrogram_data = []\r\n",
    "\r\n",
    "        # Lowest number of columns found so far\r\n",
    "        lowest_num_col = np.Inf\r\n",
    "\r\n",
    "        # For every window size (in ms)\r\n",
    "        for window_size in window_sizes:\r\n",
    "\r\n",
    "            # Calculate parameters for Short Time Fourier Transform (STFT)\r\n",
    "            n_fft      = int(round(window_size * sample_rate / 1e3))\r\n",
    "            hop_length = int(round(     stride * sample_rate / 1e3))\r\n",
    "\r\n",
    "            # Spectrogram is generated\r\n",
    "            spectrogram = librosa.feature.melspectrogram(raw_audio, sr=sample_rate, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels)\r\n",
    "\r\n",
    "            # Spectrogram gets scaled into decibels\r\n",
    "            spectrogram_db = librosa.power_to_db(spectrogram, ref=np.max)\r\n",
    "\r\n",
    "            # Keep a record of the lowest number of columns found\r\n",
    "            if spectrogram_db.shape[1] <= lowest_num_col:\r\n",
    "                lowest_num_col = spectrogram_db.shape[1]\r\n",
    "\r\n",
    "            # Reduce size of array in case the current number of columns is higher than the\r\n",
    "            # lowest recorded number of columns.\r\n",
    "            else:\r\n",
    "                sys.stderr.flush()\r\n",
    "                print(\"Spectrogram with a higher dimensionality found. Slicing to match the remaining sequences.\")\r\n",
    "                spectrogram_db = spectrogram_db[:,0:lowest_num_col]\r\n",
    "\r\n",
    "            # New data is appended\r\n",
    "            spectrogram_data.append(spectrogram_db)\r\n",
    "\r\n",
    "        # ===================\r\n",
    "        # SAVING AUDIO DATA\r\n",
    "        # ===================\r\n",
    "\r\n",
    "        # Resize array to have the shape: (Time x N Mel x 3)\r\n",
    "        spectrogram_data = np.reshape(np.array(spectrogram_data), (-1, n_mels, 3))\r\n",
    "\r\n",
    "        # Equally spaced values between 0 and the number of frames in the STFT\r\n",
    "        k = np.linspace(0, spectrogram_data[0], spectrogram_data.shape[0])\r\n",
    "\r\n",
    "        # Convert the spectrogram frames into seconds (timestamps)\r\n",
    "        # (Get in which second a frame occurs)\r\n",
    "        time_data = librosa.core.frames_to_time(k, sample_rate, hop_length)\r\n",
    "\r\n",
    "        # Add the current \"spectrogram matrix\" to \"audio_data\"\r\n",
    "        audio_data[pack_name][song_name] = {\"spectrogram\": spectrogram_data, \"time\": time_data}\r\n",
    "\r\n",
    "# The resulting audio data is stored in a pickle file\r\n",
    "with open('audio_data.pickle', 'wb') as handle:\r\n",
    "    pickle.dump(audio_data, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Fraxtil's Arrow Arrangements: 100%|██████████| 20/20 [05:09<00:00, 15.48s/it]\n",
      "Fraxtil's Beast Beats: 100%|██████████| 20/20 [04:20<00:00, 13.03s/it]\n",
      "In The Groove:  85%|████████▌ | 57/67 [10:47<02:03, 12.38s/it]Spectrogram with a higher dimensionality found. Slicing to match the remaining sequences.\n",
      "In The Groove: 100%|██████████| 67/67 [12:31<00:00, 11.22s/it]\n",
      "In The Groove 2:  15%|█▌        | 10/66 [01:26<08:00,  8.58s/it]Spectrogram with a higher dimensionality found. Slicing to match the remaining sequences.\n",
      "In The Groove 2:  39%|███▉      | 26/66 [03:53<06:21,  9.54s/it]Spectrogram with a higher dimensionality found. Slicing to match the remaining sequences.\n",
      "In The Groove 2: 100%|██████████| 66/66 [10:06<00:00,  9.18s/it]\n",
      "KDA - ALL OUT: 100%|██████████| 7/7 [01:12<00:00, 10.40s/it]\n",
      "Tsunamix III:  60%|██████    | 30/50 [06:07<04:08, 12.41s/it]Spectrogram with a higher dimensionality found. Slicing to match the remaining sequences.\n",
      "Tsunamix III:  66%|██████▌   | 33/50 [06:42<03:18, 11.70s/it]Spectrogram with a higher dimensionality found. Slicing to match the remaining sequences.\n",
      "Tsunamix III: 100%|██████████| 50/50 [10:05<00:00, 12.11s/it]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load Audio Data (Alternative)\r\n",
    "\r\n",
    "If the audio data generation was run before, the user can re-load all the previously processed assets."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "# Load the audio data (deserialize)\r\n",
    "with open('audio_data.pickle', 'rb') as file:\r\n",
    "    audio_data = pickle.load(file)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Weights and Biases (WandB) Setup"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "source": [
    "# RemoveS excesive notifications from WandB during training\r\n",
    "logger = logging.getLogger(\"wandb\")\r\n",
    "logger.setLevel(logging.ERROR)\r\n",
    "\r\n",
    "# Name of the current notebook according to WandB\r\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"Dance Dance Convolution - Revisited.ipynb\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Expansion\r\n",
    "\r\n",
    "Charts can be flipped horizontally (mirrored) and their choreography will not be altered in any way. To add aditional training samples we flip all charts."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# PENDING"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prepare the Data (Reshape and Train/Test/Valid Splits)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# PENDING"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Keras Imports"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "source": [
    "# Keras\r\n",
    "# NOTA: Do not mix tensorflow and keras imports for layers or\r\n",
    "# optimizers. During compilation the API will return an error.\r\n",
    "import keras\r\n",
    "from keras.preprocessing.image import ImageDataGenerator\r\n",
    "from tensorflow.keras.utils import to_categorical\r\n",
    "from keras.optimizers import adam_v2\r\n",
    "from keras.models import Sequential\r\n",
    "from keras.layers import Dense\r\n",
    "from keras.layers import Conv2D\r\n",
    "from keras.layers import MaxPool1D\r\n",
    "from keras.layers import Flatten\r\n",
    "from keras.layers import Input\r\n",
    "from keras.layers import Bidirectional\r\n",
    "from keras.layers import LSTM\r\n",
    "from keras.layers import Concatenate"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Keras Model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "source": [
    "# WandB: 0. Weights and Biases login (only if its your first time using it)\r\n",
    "# wandb.login()\r\n",
    "\r\n",
    "# WandB: 1. New run declaration with all the parameters to track.\r\n",
    "run = wandb.init(project=\"DanceDanceConvolutionX\", entity=\"sanoli\",\r\n",
    "                 config={\r\n",
    "                    \"learning_rate\": 0.001, \r\n",
    "                    \"epochs\": 10, \r\n",
    "                    \"batch_size\": 32,  \r\n",
    "                    \"loss_function\": {\r\n",
    "                       \"OUT_Stp\": \"binary_crossentropy\",\r\n",
    "                       \"OUT_bpm\": \"mse\",\r\n",
    "                       \"OUT_PLeft\" : \"categorical_crossentropy\",\r\n",
    "                       \"OUT_PRight\": \"categorical_crossentropy\",\r\n",
    "                       \"OUT_PUp\"   : \"categorical_crossentropy\",\r\n",
    "                       \"OUT_PDown\" : \"categorical_crossentropy\"\r\n",
    "                    },\r\n",
    "                    \"loss_weights\": {\r\n",
    "                       \"OUT_Stp\": 1,\r\n",
    "                       \"OUT_bpm\": 1,\r\n",
    "                       \"OUT_PLeft\" : 1,\r\n",
    "                       \"OUT_PRight\": 1,\r\n",
    "                       \"OUT_PUp\"   : 1,\r\n",
    "                       \"OUT_PDown\" : 1\r\n",
    "                    },\r\n",
    "                    \"architecture\": \"CNN+RNN+MLP\",  \r\n",
    "                    \"dataset\": \"DDR Songs\"\r\n",
    "                 })\r\n",
    "\r\n",
    "\r\n",
    "# Base structure for the network\r\n",
    "model = Sequential()\r\n",
    "\r\n",
    "# Network Weight initialization\r\n",
    "# It makes use of the 'glorot' initializer, also known as Xavier's initializer\r\n",
    "initializer = keras.initializers.GlorotNormal()\r\n",
    "\r\n",
    "# Model layers\r\n",
    "# 1. Convolutional Section\r\n",
    "#    NOTE: The last concatenated layer adds a OHE of the charts difficulty\r\n",
    "IN  = Input(shape=(15, 80, 3))\r\n",
    "CL1 = Conv2D(filters=10, kernel_size=(7,3), strides=1, activation=\"relu\", padding=\"same\")(IN)\r\n",
    "MP1 = MaxPool1D(pool_size=3, strides=3)(CL1)\r\n",
    "CL2 = Conv2D(filters=20, kernel_size=(3,3), strides=1, activation=\"relu\", padding=\"same\")(MP1)\r\n",
    "MP2 = MaxPool1D(pool_size=3, strides=3)(CL2)\r\n",
    "F1  = Flatten()(MP2)\r\n",
    "CC1 = Concatenate(axis=1)([F1, [0,0,0,0,1]])\r\n",
    "\r\n",
    "# 2. First Fully Connected Section\r\n",
    "D1  = Dense(units=256, activation=\"relu\", kernel_initializer=initializer)(CC1)\r\n",
    "D2  = Dense(units=128, activation=\"relu\", kernel_initializer=initializer)(D1)\r\n",
    "\r\n",
    "# 3. Recurrent Section\r\n",
    "BL1 = Bidirectional(LSTM(200, return_sequences=True))(D2)\r\n",
    "BL2 = Bidirectional(LSTM(200, return_sequences=True))(BL1)\r\n",
    "BL3 = Bidirectional(LSTM(200, return_sequences=True))(BL2)\r\n",
    "\r\n",
    "# 4. Final Fully Connected Section\r\n",
    "D3  = Dense(units=128, activation=\"relu\", kernel_initializer=initializer)(BL3)\r\n",
    "D4  = Dense(units= 64, activation=\"relu\", kernel_initializer=initializer)(D3)\r\n",
    "\r\n",
    "# 5 . Different Output for Each Part of the Prediction\r\n",
    "# y[0]     = Probability of step. Sigmoid\r\n",
    "# y[1]     = BPM value for current step. ReLu\r\n",
    "# y[2:10]  = Probability distribution of \"Left\" step (One for each note type. 9 in total). Softmax\r\n",
    "# y[11:19] = Probability distribution of \"Down\" step (One for each note type. 9 in total). Softmax\r\n",
    "# y[20:28] = Probability distribution of \"Up\" step (One for each note type. 9 in total). Softmax\r\n",
    "# y[29:37] = Probability distribution of \"Right\" step (One for each note type. 9 in total). Softmax\r\n",
    "OUT_Stp = Dense(units=1, activation=\"sigmoid\")(D4)\r\n",
    "OUT_bpm = Dense(units=1, activation=\"relu\")(D4)\r\n",
    "OUT_PLeft  = Dense(units=9, activation=\"softmax\")(D4)\r\n",
    "OUT_PRight = Dense(units=9, activation=\"softmax\")(D4)\r\n",
    "OUT_PUp    = Dense(units=9, activation=\"softmax\")(D4)\r\n",
    "OUT_PDown  = Dense(units=9, activation=\"softmax\")(D4)\r\n",
    "\r\n",
    "# WandB: 2. Stores the inputs and hyperparameters of the model\r\n",
    "config = wandb.config\r\n",
    "\r\n",
    "# The optimizer is selected\r\n",
    "optimizer = adam_v2.Adam(learning_rate=config.learning_rate)\r\n",
    "\r\n",
    "# The model is compiled\r\n",
    "model.compile(optimizer=optimizer, metrics=[\"accuracy\"], \r\n",
    "              loss=config.loss_function, loss_weights=config.loss_weights)\r\n",
    "model.summary()"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "Finishing last run (ID:2863b2ms) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 23020<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4edaf82cfe144faea80eef34105178e6"
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>e:\\Archivos\\Educación\\Posgrado\\Universidad Galileo (UG)\\Trimestre III\\Statistical Learning II\\Proyecto\\DDC Revisited\\wandb\\run-20210924_234257-2863b2ms\\logs\\debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>e:\\Archivos\\Educación\\Posgrado\\Universidad Galileo (UG)\\Trimestre III\\Statistical Learning II\\Proyecto\\DDC Revisited\\wandb\\run-20210924_234257-2863b2ms\\logs\\debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">confused-glitter-1</strong>: <a href=\"https://wandb.ai/sanoli/DanceDanceConvolutionX/runs/2863b2ms\" target=\"_blank\">https://wandb.ai/sanoli/DanceDanceConvolutionX/runs/2863b2ms</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "...Successfully finished last run (ID:2863b2ms). Initializing new run:<br/><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "wandb: wandb version 0.12.2 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.12.1<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">clean-silence-2</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/sanoli/DanceDanceConvolutionX\" target=\"_blank\">https://wandb.ai/sanoli/DanceDanceConvolutionX</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/sanoli/DanceDanceConvolutionX/runs/2hmsi16f\" target=\"_blank\">https://wandb.ai/sanoli/DanceDanceConvolutionX/runs/2hmsi16f</a><br/>\n",
       "                Run data is saved locally in <code>e:\\Archivos\\Educación\\Posgrado\\Universidad Galileo (UG)\\Trimestre III\\Statistical Learning II\\Proyecto\\DDC Revisited\\wandb\\run-20210924_234331-2hmsi16f</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "Input 0 of layer max_pooling1d is incompatible with the layer: expected ndim=3, found ndim=4. Full shape received: (None, 15, 80, 10)",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-50-66a789247d7d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[0mIN\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m80\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[0mCL1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mConv2D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"relu\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"same\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mIN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m \u001b[0mMP1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMaxPool1D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpool_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCL1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m \u001b[0mCL2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mConv2D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"relu\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"same\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMP1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[0mMP2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMaxPool1D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpool_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCL2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    974\u001b[0m     \u001b[1;31m# >> model = tf.keras.Model(inputs, outputs)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    975\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0m_in_functional_construction_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 976\u001b[1;33m       return self._functional_construction_call(inputs, args, kwargs,\n\u001b[0m\u001b[0;32m    977\u001b[0m                                                 input_list)\n\u001b[0;32m    978\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m_functional_construction_call\u001b[1;34m(self, inputs, args, kwargs, input_list)\u001b[0m\n\u001b[0;32m   1112\u001b[0m         layer=self, inputs=inputs, build_graph=True, training=training_value):\n\u001b[0;32m   1113\u001b[0m       \u001b[1;31m# Check input assumptions set after layer building, e.g. input shape.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1114\u001b[1;33m       outputs = self._keras_tensor_symbolic_call(\n\u001b[0m\u001b[0;32m   1115\u001b[0m           inputs, input_masks, args, kwargs)\n\u001b[0;32m   1116\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m_keras_tensor_symbolic_call\u001b[1;34m(self, inputs, input_masks, args, kwargs)\u001b[0m\n\u001b[0;32m    846\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeras_tensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mKerasTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_signature\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    847\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 848\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_infer_output_signature\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    850\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_infer_output_signature\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m_infer_output_signature\u001b[1;34m(self, inputs, args, kwargs, input_masks)\u001b[0m\n\u001b[0;32m    884\u001b[0m           \u001b[1;31m# overridden).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    885\u001b[0m           \u001b[1;31m# TODO(kaftan): do we maybe_build here, or have we already done it?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 886\u001b[1;33m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_build\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    887\u001b[0m           \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_cast_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m           \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m_maybe_build\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2631\u001b[0m     \u001b[1;31m# Check input assumptions set before layer building, e.g. input rank.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2632\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2633\u001b[1;33m       input_spec.assert_input_compatibility(\n\u001b[0m\u001b[0;32m   2634\u001b[0m           self.input_spec, inputs, self.name)\n\u001b[0;32m   2635\u001b[0m       \u001b[0minput_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[1;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[0;32m    212\u001b[0m       \u001b[0mndim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrank\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mndim\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mspec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 214\u001b[1;33m         raise ValueError('Input ' + str(input_index) + ' of layer ' +\n\u001b[0m\u001b[0;32m    215\u001b[0m                          \u001b[0mlayer_name\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' is incompatible with the layer: '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    216\u001b[0m                          \u001b[1;34m'expected ndim='\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m', found ndim='\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Input 0 of layer max_pooling1d is incompatible with the layer: expected ndim=3, found ndim=4. Full shape received: (None, 15, 80, 10)"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model Training"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Se entrena el modelo \r\n",
    "model.fit(X_train, y_train, epochs=config.epochs, batch_size=config.batch_size, validation_data=(X_valid, y_valid), \r\n",
    "       callbacks=[\r\n",
    "            # Se envían datos a weights and biases\r\n",
    "            WandbCallback(\r\n",
    "                data_type=\"image\",                      # Se generan imágenes en el reporte\r\n",
    "                monitor=\"accuracy\",                     # Monitorea el accuracy como métrica\r\n",
    "                mode=\"max\",                             # Trackea aumentos en accuracy\r\n",
    "                save_model=True,                        # Guardar modelo cuando se alcanza un nuevo máximo en accuracy\r\n",
    "                validation_data=(X_valid, y_valid),     # WandB hace predicciones a medio proceso y las despliega en el dashboard\r\n",
    "            ),          \r\n",
    "       ])\r\n",
    "\r\n",
    "# Se mide la precisión con set pruebas\r\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\r\n",
    "print(f\"Test Error Rate: {round((1-accuracy)*100, 2)}\")\r\n",
    "\r\n",
    "# Se loguean los resultados en WandB\r\n",
    "wandb.log({\"Test Error Rate\" : round((1-accuracy)*100, 2)})\r\n",
    "run.join()\r\n",
    "\r\n",
    "# Se finaliza el \"run\" de WandB\r\n",
    "run.finish()"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.2",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.2 64-bit"
  },
  "interpreter": {
   "hash": "658dc12c475a3a8caebf03b24f414cffa2901ebd330ffd26b9c22f028a90850c"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}