{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Dance Dance Convolution - Revisited\r\n",
    "\r\n",
    "Revision of the original \"Dance Dance Convolution\" paper, that incorporates newer machine learning and AI techniques to \"hopefully\" improve the original model results."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Imports"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import os\r\n",
    "import sys\r\n",
    "import pickle\r\n",
    "import logging\r\n",
    "from tqdm import tqdm\r\n",
    "from pathlib import Path\r\n",
    "from collections import defaultdict\r\n",
    "from os.path import isfile, join, splitext, basename, normpath, exists\r\n",
    "\r\n",
    "import numpy as np\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import librosa\r\n",
    "import librosa.display\r\n",
    "\r\n",
    "# Weights and Biases (WandB)\r\n",
    "import wandb \r\n",
    "from wandb.keras import WandbCallback\r\n",
    "\r\n",
    "# Keras\r\n",
    "# IMPORTANT: Do not mix tensorflow and keras imports for layers or optimizers.\r\n",
    "import keras\r\n",
    "from tensorflow.keras.utils import to_categorical\r\n",
    "from keras.optimizers import adam_v2\r\n",
    "from keras.models import Sequential\r\n",
    "from keras.layers import Dense\r\n",
    "from keras.layers import Conv2D\r\n",
    "from keras.layers import MaxPool2D\r\n",
    "from keras.layers import Flatten\r\n",
    "from keras.layers import Input\r\n",
    "from keras.layers import Bidirectional\r\n",
    "from keras.layers import LSTM\r\n",
    "from keras.layers import Concatenate\r\n",
    "from keras.layers import TimeDistributed\r\n",
    "from keras.models import Model\r\n",
    "\r\n",
    "# Custom Functions\r\n",
    "from sm_parsing import stepfile_parser\r\n",
    "from post_process import add_measure_timestamps, log_spectrogram"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loading and Cleaning Dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# Path to dataset\r\n",
    "base_path = \"./dataset\"\r\n",
    "\r\n",
    "# Extension of the required stepfile\r\n",
    "stepfile_ext = \".sm\"\r\n",
    "\r\n",
    "# Allowed audio and stepfile extensions\r\n",
    "audio_exts = [\".ogg\", \".mp3\", \".wav\"]\r\n",
    "steps_exts = [\".sm\", \".ssc\"]\r\n",
    "\r\n",
    "# Song packs inside dataset\r\n",
    "# (Ignores files that are not directories)\r\n",
    "song_packs = [f for f in os.listdir(base_path) if not isfile(join(base_path, f))]\r\n",
    "\r\n",
    "print(f\"Found a total of {len(song_packs)} song packs.\")\r\n",
    "\r\n",
    "# =======================================\r\n",
    "# SONG PACK CLEANING AND DATA EXTRACTION\r\n",
    "# =======================================\r\n",
    "\r\n",
    "# Files that dont add anything to training are deleted (videos, images, txts, etc.)\r\n",
    "# Wanted file extensions\r\n",
    "wanted_ext = audio_exts + steps_exts\r\n",
    "\r\n",
    "# Counter for the number of files deleted.\r\n",
    "files_deleted = 0\r\n",
    "\r\n",
    "# Counter for the number of songs encountered\r\n",
    "songs_encountered = 0\r\n",
    "\r\n",
    "# Dictionary that will get one entry for each pack\r\n",
    "pack_data = {}\r\n",
    "\r\n",
    "# For every song pack\r\n",
    "for pack_name in song_packs:\r\n",
    "\r\n",
    "    # Empty dict that stores all relevant filepaths for a song inside a pack\r\n",
    "    # All unseen keys are assigned an empty list by default\r\n",
    "    song_data = defaultdict(lambda: [])\r\n",
    "\r\n",
    "    # Go through every file in the song pack\r\n",
    "    # (including files and subfiles)\r\n",
    "    for path, _, files in os.walk(join(base_path, pack_name)):\r\n",
    "\r\n",
    "        # For every file inside the base path\r\n",
    "        for file in files: \r\n",
    "\r\n",
    "            # Get the current file's parent folder (song folder)\r\n",
    "            # 1. The absolute path for the parent directory is extracted\r\n",
    "            # 2. 'normpath' strips off any trailing slashes\r\n",
    "            # 3. 'basename' returns the last part of the path\r\n",
    "            parent_name = basename(normpath(Path(path)))\r\n",
    "\r\n",
    "            # If the file has a \"pack_name\" as a parent the file is outside \r\n",
    "            # a song folder, it is ignored as a result.\r\n",
    "            if parent_name in song_packs:\r\n",
    "                print(f\"Found '{file}' outside of a song folder. Ignoring file.\")\r\n",
    "                continue\r\n",
    "\r\n",
    "            # The file extension is extracted\r\n",
    "            _, ext = splitext(file)\r\n",
    "\r\n",
    "            # File is deleted if it has an unwanted extension\r\n",
    "            if ext not in wanted_ext:\r\n",
    "                try:\r\n",
    "                    os.remove(join(path, file))\r\n",
    "                    files_deleted += 1\r\n",
    "                except Exception as e:\r\n",
    "                    raise Exception(e)\r\n",
    "\r\n",
    "            # All the paths that relate to a song are stored in a dict\r\n",
    "            # according to their name and the songpack they belong to\r\n",
    "            else:\r\n",
    "                song_data[parent_name].append(join(path, file))\r\n",
    "    \r\n",
    "    # The \"song_data\" is stored inside the \"pack_data\"\r\n",
    "    # (This is to prevent two packs having the same title for a\r\n",
    "    # song and risking overwriting the data for one song.)\r\n",
    "    pack_data[pack_name] = song_data\r\n",
    "\r\n",
    "    # We add the number of songs in the pack to \"songs_encountered\"\r\n",
    "    songs_encountered += len(list(song_data.keys()))\r\n",
    "\r\n",
    "# Printout after cleaning\r\n",
    "if files_deleted == 0:\r\n",
    "    print(f\"Dataset already clean. {songs_encountered} songs found. 0 files deleted.\")\r\n",
    "else:\r\n",
    "    print(f\"Dataset cleaned successfully. {songs_encountered} songs found. {files_deleted} files deleted.\")\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Found a total of 6 song packs.\n",
      "Found 'group.ini' outside of a song folder. Ignoring file.\n",
      "Dataset already clean. 230 songs found. 0 files deleted.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Check if Songs Have Both Audio and Note Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# Dict for the path of all audio files\r\n",
    "audio_paths = defaultdict(dict)\r\n",
    "\r\n",
    "# Number of songs with both audio and a stepfile\r\n",
    "complete_songs = 0\r\n",
    "\r\n",
    "# For every song in each pack\r\n",
    "for pack_name in pack_data.keys():\r\n",
    "    for song_name in pack_data[pack_name].keys():\r\n",
    "\r\n",
    "        # We get all the extensions found for a song\r\n",
    "        song_folder_exts = [splitext(path)[1] for path in pack_data[pack_name][song_name]]\r\n",
    "\r\n",
    "        # Check one or more audio extensions were found inside the song folder\r\n",
    "        audio_check = any([audio_ext in song_folder_exts for audio_ext in audio_exts])\r\n",
    "\r\n",
    "        # Check if the required stepfile extension was found\r\n",
    "        sm_check = stepfile_ext in song_folder_exts\r\n",
    "\r\n",
    "        # If the song doesnt pass both checks, the song gets deleted from the dict\r\n",
    "        if not(audio_check and sm_check):\r\n",
    "\r\n",
    "            del pack_data[pack_name][song_name]\r\n",
    "            print(f\"Song '{song_name}' of pack '{pack_name}' does not contain one of the required files for training. Removing song from dataset.\")\r\n",
    "\r\n",
    "        else:\r\n",
    "            # We extract the path for the songs audio file\r\n",
    "            # A path is extracted only if it has one of the required extensions\r\n",
    "            audio_path = [path for path in pack_data[pack_name][song_name] if splitext(path)[1] in audio_exts]\r\n",
    "\r\n",
    "            # The path is added to an \"audio files\" dict\r\n",
    "            audio_paths[pack_name][song_name] = audio_path\r\n",
    "\r\n",
    "            # Increase the number of complete songs by one\r\n",
    "            complete_songs += 1\r\n",
    "\r\n",
    "\r\n",
    "print(f\"Complete songs: {complete_songs} / {songs_encountered} (Contained both audio and a stepfile file).\")\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Complete songs: 230 / 230 (Contained both audio and a stepfile file).\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tag Parsing Stepfiles"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# Counter for songs successfully processed\r\n",
    "successfully_processed = 0\r\n",
    "\r\n",
    "# Dict to store the tag data for each song in the pack \r\n",
    "tag_data = defaultdict(dict)\r\n",
    "\r\n",
    "# For every pack and song in the dataset\r\n",
    "for pack_name in pack_data.keys():\r\n",
    "    for song_name in tqdm(pack_data[pack_name], desc=f\"{pack_name}\"): \r\n",
    "\r\n",
    "        try:\r\n",
    "            # For every path corresponding to the current song, we take\r\n",
    "            # the one that contains the extension that we need. Due to it being\r\n",
    "            # returned inside of a list, we get the first element.\r\n",
    "            stepfile_path = [path for path in pack_data[pack_name][song_name] if stepfile_ext in path][0]\r\n",
    "\r\n",
    "        # If an error occurs while getting the stepfile path,\r\n",
    "        # the program skips the current song\r\n",
    "        except Exception:\r\n",
    "            print(f\"No '{stepfile_ext}' file found for song '{song_name}' in song pack '{pack_name}'. Skipping song.\")\r\n",
    "            continue\r\n",
    "\r\n",
    "        # Step file content is extracted as text\r\n",
    "        with open(stepfile_path, 'r', encoding=\"utf-8\") as stepfile:\r\n",
    "            stepfile_txt = stepfile.read()\r\n",
    "            \r\n",
    "        # The text of each song is parsed and turned into a dict of tags\r\n",
    "        tag_data[pack_name][song_name] = stepfile_parser(stepfile_txt)\r\n",
    "\r\n",
    "        # Required tags\r\n",
    "        required_tags = ['offset', 'bpms', 'notes']\r\n",
    "\r\n",
    "        # Current song tags\r\n",
    "        current_tags = list(tag_data[pack_name][song_name].keys())\r\n",
    "\r\n",
    "        # Check if resulting dictionary keys contain all the required tags\r\n",
    "        if not all((item in current_tags) for item in required_tags):\r\n",
    "            raise Exception(f\"Song '{song_name}' of pack '{pack_name}' does not contain all of the required tags: 'offset', 'bpms' and 'notes'.\")\r\n",
    "\r\n",
    "        # Increase the number of files succesfully processed\r\n",
    "        else:\r\n",
    "            successfully_processed += 1\r\n",
    "\r\n",
    "\r\n",
    "# Tag data is saved to a pickle file\r\n",
    "with open('tag_data.pickle', 'wb') as handle:\r\n",
    "    pickle.dump(tag_data, handle, protocol=pickle.HIGHEST_PROTOCOL)\r\n",
    "    print(\"Tag data successfully saved.\")\r\n",
    "\r\n",
    "# Successful files\r\n",
    "print(f\"Number of succesfully processed songs: {successfully_processed} / {songs_encountered}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Fraxtil's Arrow Arrangements: 100%|██████████| 20/20 [00:17<00:00,  1.17it/s]\n",
      "Fraxtil's Beast Beats: 100%|██████████| 20/20 [00:17<00:00,  1.16it/s]\n",
      "In The Groove: 100%|██████████| 67/67 [00:47<00:00,  1.43it/s]\n",
      "In The Groove 2: 100%|██████████| 66/66 [00:36<00:00,  1.82it/s]\n",
      "KDA - ALL OUT: 100%|██████████| 7/7 [00:03<00:00,  2.32it/s]\n",
      "Tsunamix III: 100%|██████████| 50/50 [00:13<00:00,  3.72it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of succesfully processed songs: 230 / 230\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Alternative: Load Tag Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# Load the tag data (deserialize)\r\n",
    "with open('tag_data.pickle', 'rb') as file:\r\n",
    "    tag_data = pickle.load(file)\r\n",
    "    print(\"Tag data succesfully loaded.\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Tag data succesfully loaded.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Add Timestamps to all Charts"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# Dictionary to store the processed data\r\n",
    "measure_data = defaultdict(dict)\r\n",
    "\r\n",
    "# For every pack and song in the dataset\r\n",
    "for pack_name in tag_data.keys():\r\n",
    "    for song_name in tag_data[pack_name].keys(): \r\n",
    "\r\n",
    "        # We store the post-processed measures\r\n",
    "        measure_data[pack_name][song_name] = add_measure_timestamps(tag_data[pack_name][song_name])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Audio to Spectrogram\r\n",
    "\r\n",
    "Based off the following article: https://towardsdatascience.com/getting-to-know-the-mel-spectrogram-31bca3e2d9d0"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Dict for all spectrogram data (all songs)\r\n",
    "audio_data = defaultdict(dict)\r\n",
    "\r\n",
    "# For every pack and song in the dataset\r\n",
    "for pack_name in tag_data.keys():\r\n",
    "\r\n",
    "    # We use \"file=sys.stdout\" to make the output nicer\r\n",
    "    for song_name in tqdm(tag_data[pack_name].keys(), desc=f\"{pack_name}\", file=sys.stdout): \r\n",
    "\r\n",
    "        # ===================\r\n",
    "        # LOADING AUDIO\r\n",
    "        # ===================\r\n",
    "\r\n",
    "        # Extract the path to the song's audio\r\n",
    "        audio_path = audio_paths[pack_name][song_name][0]\r\n",
    "\r\n",
    "        # Audio gets loaded\r\n",
    "        raw_audio, sample_rate = librosa.load(audio_path)\r\n",
    "\r\n",
    "        # ===================\r\n",
    "        # SPECTROGRAM (STFT)\r\n",
    "        # ===================\r\n",
    "\r\n",
    "        # Hyperparameters for Librosas's STFT\r\n",
    "        # Both the window size and the stride are given in miliseconds.\r\n",
    "        # \"n_mels\" consists of the number of frequency bins that the user after applying the \"Mel Scale\".\r\n",
    "        # We use three different window sizes to capture different amounts of \"detail\" in the signal.\r\n",
    "        window_sizes = [23, 46, 93]            \r\n",
    "        stride = 10\r\n",
    "        n_mels = 80\r\n",
    "\r\n",
    "        # 3D tensor for the all the STFT results after using each window size\r\n",
    "        spectrogram_data = []\r\n",
    "\r\n",
    "        # Lowest number of columns found so far\r\n",
    "        lowest_num_col = np.Inf\r\n",
    "\r\n",
    "        # For every window size (in ms)\r\n",
    "        for window_size in window_sizes:\r\n",
    "\r\n",
    "            # Calculate parameters for Short Time Fourier Transform (STFT)\r\n",
    "            n_fft      = int(round(window_size * sample_rate / 1e3))\r\n",
    "            hop_length = int(round(     stride * sample_rate / 1e3))\r\n",
    "\r\n",
    "            # Spectrogram is generated\r\n",
    "            spectrogram = librosa.feature.melspectrogram(raw_audio, sr=sample_rate, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels)\r\n",
    "\r\n",
    "            # Spectrogram gets scaled into decibels\r\n",
    "            spectrogram_db = librosa.power_to_db(spectrogram, ref=np.max)\r\n",
    "\r\n",
    "            # Keep a record of the lowest number of columns found\r\n",
    "            if spectrogram_db.shape[1] <= lowest_num_col:\r\n",
    "                lowest_num_col = spectrogram_db.shape[1]\r\n",
    "\r\n",
    "            # Reduce size of array in case the current number of columns is higher than the\r\n",
    "            # lowest recorded number of columns.\r\n",
    "            else:\r\n",
    "                sys.stderr.flush()\r\n",
    "                print(\"Spectrogram with a higher dimensionality found. Slicing to match the remaining sequences.\")\r\n",
    "                spectrogram_db = spectrogram_db[:,0:lowest_num_col]\r\n",
    "\r\n",
    "            # New data is appended\r\n",
    "            spectrogram_data.append(spectrogram_db)\r\n",
    "\r\n",
    "        # ===================\r\n",
    "        # SAVING AUDIO DATA\r\n",
    "        # ===================\r\n",
    "\r\n",
    "        # Resize array to have the shape: (Time x N Mel x 3)\r\n",
    "        spectrogram_data = np.reshape(np.array(spectrogram_data), (-1, n_mels, 3))\r\n",
    "\r\n",
    "        # Equally spaced values between 0 and the number of frames in the STFT\r\n",
    "        k = np.linspace(0, spectrogram_data[0], spectrogram_data.shape[0])\r\n",
    "\r\n",
    "        # Convert the spectrogram frames into seconds (timestamps)\r\n",
    "        # (Get in which second a frame occurs)\r\n",
    "        time_data = librosa.core.frames_to_time(k, sample_rate, hop_length)\r\n",
    "\r\n",
    "        # Add the current \"spectrogram matrix\" to \"audio_data\"\r\n",
    "        audio_data[pack_name][song_name] = {\"spectrogram\": spectrogram_data, \"time\": time_data}\r\n",
    "\r\n",
    "# The resulting audio data is stored in a pickle file\r\n",
    "with open('audio_data.pickle', 'wb') as handle:\r\n",
    "    pickle.dump(audio_data, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Alternative: Load Audio Data\r\n",
    "\r\n",
    "If the audio data generation was run before, the user can re-load all the previously processed assets."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# Load the audio data (deserialize)\r\n",
    "with open('audio_data.pickle', 'rb') as file:\r\n",
    "    audio_data = pickle.load(file)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Weights and Biases (WandB) Setup"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# RemoveS excesive notifications from WandB during training\r\n",
    "logger = logging.getLogger(\"wandb\")\r\n",
    "logger.setLevel(logging.ERROR)\r\n",
    "\r\n",
    "# Make the WandB stdout shut up\r\n",
    "os.environ[\"WANDB_SILENT\"] = \"True\"\r\n",
    "\r\n",
    "# Name of the current notebook according to WandB\r\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"Dance Dance Convolution - Revisited.ipynb\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataset Creation and Augmentation\r\n",
    "\r\n",
    "The spectrogram data is used to create the input of the neural network ($X$), while the step data is used to generate the output data ($y$). The data is doubled (augmented) by flipping horizontally all steps. This is a suitable \"augmentation\" as it does not destroy the underlying patterns in the songs. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\r\n",
    "\r\n",
    "# ======================\r\n",
    "# STEP CLASS ENCODING\r\n",
    "# ======================\r\n",
    "\r\n",
    "# There are 9 different types of step. We encode for each one:\r\n",
    "# 0 - No Note\r\n",
    "# 1 - Normal Note\r\n",
    "# 2 - Hold Head\r\n",
    "# 3 - Hold/Roll Tail\r\n",
    "# 4 - Roll Head\r\n",
    "# M - Mine (or \"bad\" note)\r\n",
    "# K - Automatic keysound\r\n",
    "# L - Lift note\r\n",
    "# F - Fake note\r\n",
    "num_step_classes = 9\r\n",
    "\r\n",
    "# One hot encoder fit for detecting the 9 step classes\r\n",
    "OHEnc_step = OneHotEncoder(handle_unknown='ignore')\r\n",
    "OHEnc_step.fit(np.reshape(np.arange(0, num_step_classes), (-1,1)))\r\n",
    "\r\n",
    "# One hot encoder for the difficulty\r\n",
    "difficulties = np.array([\"Beginner\", \"Easy\", \"Medium\", \"Hard\", \"Challenge\", \"Edit\"])\r\n",
    "OHEnc_diff = OneHotEncoder(handle_unknown='ignore')\r\n",
    "OHEnc_diff.fit(np.reshape(difficulties, (-1,1)))\r\n",
    "\r\n",
    "# Empty lists for the inputs and outputs\r\n",
    "X_data = []\r\n",
    "y_data = []\r\n",
    "\r\n",
    "# Iterate once again over every pack and song\r\n",
    "# We use \"file=sys.stdout\" to make the output nicer\r\n",
    "for pack in tag_data.keys():\r\n",
    "    for song in tqdm(tag_data[pack].keys(), desc=f\"{pack}\", file=sys.stdout): \r\n",
    "\r\n",
    "        # =======================\r\n",
    "        # MEASURE DATA (OUTPUT)\r\n",
    "        # =======================\r\n",
    "\r\n",
    "        # For every difficulty that uses a single pad\r\n",
    "        for difficulty in measure_data[pack][song][\"dance-single\"].keys():\r\n",
    "\r\n",
    "            # One hot encoding of the difficulty\r\n",
    "            difficulty_enc = OHEnc_diff.transform(np.array([[difficulty]])).toarray()\r\n",
    "            \r\n",
    "            # Extract the current difficulty chart\r\n",
    "            chart = measure_data[pack][song][\"dance-single\"][difficulty]\r\n",
    "\r\n",
    "            # Timestamp values for timing chart steps\r\n",
    "            step_timestamps = chart[:,4]\r\n",
    "\r\n",
    "            # P(Step): Probability of a step occurring\r\n",
    "            P_step = 1*np.any(chart[:,0:4], axis=1, keepdims=True)\r\n",
    "\r\n",
    "            # BPM: BPM value during each step\r\n",
    "            BPM_step = np.reshape(chart[:,5], (-1, 1))\r\n",
    "\r\n",
    "            # Probability of step class ocurring for each step direction \r\n",
    "            # - P(Left)  : Multiclass probability of a left step occurring\r\n",
    "            # - P(Down)  : Multiclass probability of a down step occurring\r\n",
    "            # - P(Up)    : Multiclass probability of a up step occurring\r\n",
    "            # - P(Right) : Multiclass probability of a right step occurring\r\n",
    "            P_left  = OHEnc_step.transform(np.reshape(chart[:,0], (-1, 1))).toarray()\r\n",
    "            P_down  = OHEnc_step.transform(np.reshape(chart[:,1], (-1, 1))).toarray()\r\n",
    "            P_up    = OHEnc_step.transform(np.reshape(chart[:,2], (-1, 1))).toarray()\r\n",
    "            P_right = OHEnc_step.transform(np.reshape(chart[:,3], (-1, 1))).toarray()\r\n",
    "\r\n",
    "            # Concatenation of the previous parts to form the output of the \r\n",
    "            # neural network for a \"singles\" song chart.\r\n",
    "            output_nn = np.hstack((P_step, BPM_step, P_left, P_down, P_up, P_right))\r\n",
    "\r\n",
    "            # Output tuple: \r\n",
    "            # - Chart data with the shape of the output of the neural net\r\n",
    "            # - Difficulty one hot encoded (to condition the output)\r\n",
    "            # - The step timestamps for timing each step\r\n",
    "            y_data.append((output_nn, difficulty_enc, step_timestamps))\r\n",
    "\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Fraxtil's Arrow Arrangements: 100%|██████████| 20/20 [00:00<00:00, 48.56it/s]\n",
      "Fraxtil's Beast Beats: 100%|██████████| 20/20 [00:00<00:00, 54.42it/s]\n",
      "In The Groove: 100%|██████████| 67/67 [00:00<00:00, 70.39it/s]\n",
      "In The Groove 2: 100%|██████████| 66/66 [00:00<00:00, 72.69it/s]\n",
      "KDA - ALL OUT: 100%|██████████| 7/7 [00:00<00:00, 76.19it/s]\n",
      "Tsunamix III: 100%|██████████| 50/50 [00:00<00:00, 74.07it/s]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "source": [
    "len(y_data)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1148"
      ]
     },
     "metadata": {},
     "execution_count": 75
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Augmentation\r\n",
    "\r\n",
    "Charts can be flipped horizontally (mirrored) and their choreography will not be altered in any way. To add aditional training samples we flip all charts."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# PENDING"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prepare the Data (Reshape and Train/Test/Valid Splits)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# PENDING"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Generator (Spectrograms)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "source": [
    "\r\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(1412, 38)"
      ]
     },
     "metadata": {},
     "execution_count": 64
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "class TimeSliceGenerator(keras.utils.Sequence):\r\n",
    "\r\n",
    "    def __init__(self, X_data, y_data, batch_size=32, shuffle=True):\r\n",
    "        pass\r\n",
    "\r\n",
    "    def on_epoch_end(self):\r\n",
    "        pass\r\n",
    "\r\n",
    "    def __getitem__(self, index):\r\n",
    "        pass\r\n",
    "\r\n",
    "    def __len__(self):\r\n",
    "        return self.n // self.batch_size"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Keras Model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "source": [
    "# ==================\r\n",
    "# SETTINGS\r\n",
    "# ==================\r\n",
    "\r\n",
    "# WandB: 0. Weights and Biases login (only if first time use)\r\n",
    "# WandB: 1. New run declaration with all the parameters to track.\r\n",
    "run = wandb.init(project=\"DanceDanceConvolutionX\", entity=\"sanoli\",\r\n",
    "                 config={\r\n",
    "                    \"learning_rate\": 0.001, \r\n",
    "                    \"epochs\": 10, \r\n",
    "                    \"batch_size\": 32,  \r\n",
    "                    \"loss_function\": {\r\n",
    "                       \"OUT_Stp\": \"binary_crossentropy\",\r\n",
    "                       \"OUT_bpm\": \"mse\",\r\n",
    "                       \"OUT_PLeft\" : \"categorical_crossentropy\",\r\n",
    "                       \"OUT_PRight\": \"categorical_crossentropy\",\r\n",
    "                       \"OUT_PUp\"   : \"categorical_crossentropy\",\r\n",
    "                       \"OUT_PDown\" : \"categorical_crossentropy\"\r\n",
    "                    },\r\n",
    "                    \"loss_weights\": {\r\n",
    "                       \"OUT_Stp\": 1,\r\n",
    "                       \"OUT_bpm\": 1,\r\n",
    "                       \"OUT_PLeft\" : 1,\r\n",
    "                       \"OUT_PRight\": 1,\r\n",
    "                       \"OUT_PUp\"   : 1,\r\n",
    "                       \"OUT_PDown\" : 1\r\n",
    "                    },\r\n",
    "                    \"architecture\": \"CNN + BiLSTM + MLP\",  \r\n",
    "                    \"dataset\": \"Fraxtil, KDA, ITG\"\r\n",
    "                 })\r\n",
    "\r\n",
    "\r\n",
    "# Network Weight initialization\r\n",
    "# It makes use of the 'glorot' initializer, also known as Xavier's initializer\r\n",
    "initializer = keras.initializers.GlorotNormal()\r\n",
    "\r\n",
    "# Number of time slices to feed the network at the same time.\r\n",
    "# (Kinda like \"time batches\")\r\n",
    "\r\n",
    "# Hyperparameters: \r\n",
    "# - batch_size: Number of samples fed to the NN at the same time\r\n",
    "# - time_batch_size: Number of time slices fed to the network at the same time\r\n",
    "batch_size = 32\r\n",
    "time_batch_size = 5\r\n",
    "\r\n",
    "# Difficulties\r\n",
    "# - The OHE is casted as a float\r\n",
    "# - We repeat the pattern as many times down as there are \"time slices\"\r\n",
    "# - We add an additional 3rd dimension to facilitate the concatenation\r\n",
    "difficulty = np.array([[0,0,0,0,1]]).astype(float)\r\n",
    "difficulty = np.tile(difficulty, (time_batch_size,1))\r\n",
    "difficulty = np.reshape(difficulty, (-1,time_batch_size, 5))\r\n",
    "\r\n",
    "# ==================\r\n",
    "# LAYERS\r\n",
    "# ==================\r\n",
    "\r\n",
    "# BLOCK 1: CONVOLUTION\r\n",
    "# - TimeDistributed is used to add a time dimension to the layers\r\n",
    "# - Rule of thumb: Dimension of convolution and max pooling has to match\r\n",
    "# - The charts difficulty (OHE) is concatenated with the flattened tensor\r\n",
    "IN  = Input(shape=(time_batch_size, 15, 80, 3), name=\"Input\")\r\n",
    "CL1 = TimeDistributed(Conv2D(filters=10, kernel_size=(7,3), strides=1, activation=\"relu\", padding=\"same\"), name=\"Conv2D-1\")(IN)\r\n",
    "MP1 = TimeDistributed(MaxPool2D(pool_size=3, strides=1), name=\"MaxPool2D-1\")(CL1)\r\n",
    "CL2 = TimeDistributed(Conv2D(filters=20, kernel_size=(3,3), strides=1, activation=\"relu\", padding=\"same\"), name=\"Conv2D-2\")(MP1)\r\n",
    "MP2 = TimeDistributed(MaxPool2D(pool_size=3, strides=3), name=\"MaxPool2D-2\")(CL2)\r\n",
    "F1  = TimeDistributed(Flatten(), name=\"Flatten\")(MP2)\r\n",
    "CC1 = Concatenate(axis=2, name=\"Concat-Diff\")([F1, difficulty])\r\n",
    "\r\n",
    "# BLOCK 2: FULLY CONNECTED\r\n",
    "D1  = Dense(units=256, activation=\"relu\", kernel_initializer=initializer, name=\"Dense-1\")(CC1)\r\n",
    "D2  = Dense(units=128, activation=\"relu\", kernel_initializer=initializer, name=\"Dense-2\")(D1)\r\n",
    "\r\n",
    "# BLOCK 3: RECURRENT NET\r\n",
    "BL1 = Bidirectional(LSTM(200, return_sequences=True), name=\"BiLSTM-1\")(D2)\r\n",
    "BL2 = Bidirectional(LSTM(200, return_sequences=True), name=\"BiLSTM-2\")(BL1)\r\n",
    "BL3 = Bidirectional(LSTM(200, return_sequences=True), name=\"BiLSTM-3\")(BL2)\r\n",
    "\r\n",
    "# BLOCK 4: FULLY CONNECTED 2 (ELECTRIC BOOGALOO)\r\n",
    "D3  = Dense(units=128, activation=\"relu\", kernel_initializer=initializer, name=\"Dense-3\")(BL3)\r\n",
    "D4  = Dense(units= 64, activation=\"relu\", kernel_initializer=initializer, name=\"Dense-4\")(D3)\r\n",
    "\r\n",
    "# BLOCK 5: SEPARATE OUTPUTS\r\n",
    "# y[0]     = Probability of step. Sigmoid\r\n",
    "# y[1]     = BPM value for current step. ReLu\r\n",
    "# y[2:10]  = Probability distribution of \"Left\" step (One for each note type. 9 in total). Softmax\r\n",
    "# y[11:19] = Probability distribution of \"Down\" step (One for each note type. 9 in total). Softmax\r\n",
    "# y[20:28] = Probability distribution of \"Up\" step (One for each note type. 9 in total). Softmax\r\n",
    "# y[29:37] = Probability distribution of \"Right\" step (One for each note type. 9 in total). Softmax\r\n",
    "OUT_Stp    = Dense(units=1, activation=\"sigmoid\", name=\"Prob-Step\")(D4)\r\n",
    "OUT_bpm    = Dense(units=1, activation=\"relu\", name=\"BPM\")(D4)\r\n",
    "OUT_PLeft  = Dense(units=9, activation=\"softmax\", name=\"Prob-Left\")(D4)\r\n",
    "OUT_PRight = Dense(units=9, activation=\"softmax\", name=\"Prob-Right\")(D4)\r\n",
    "OUT_PUp    = Dense(units=9, activation=\"softmax\", name=\"Prob-Up\")(D4)\r\n",
    "OUT_PDown  = Dense(units=9, activation=\"softmax\", name=\"Prob-Down\")(D4)\r\n",
    "\r\n",
    "# BLOCK 6: FINAL CONCATENATION\r\n",
    "OUT = Concatenate(name=\"Concat-Outs\")([OUT_Stp, OUT_bpm, OUT_PLeft, OUT_PRight, OUT_PUp, OUT_PDown])\r\n",
    "\r\n",
    "# ==================\r\n",
    "# MODEL CREATION\r\n",
    "# ==================\r\n",
    "\r\n",
    "# A model is created with all the previous parts\r\n",
    "model = Model(inputs=IN, outputs=OUT, name=\"DDCX\")\r\n",
    "\r\n",
    "# WandB: 2. Stores the inputs and hyperparameters of the model\r\n",
    "config = wandb.config\r\n",
    "\r\n",
    "# The optimizer is selected\r\n",
    "optimizer = adam_v2.Adam(learning_rate=config.learning_rate)\r\n",
    "\r\n",
    "# The model is compiled\r\n",
    "model.compile(optimizer=optimizer, metrics=[\"accuracy\"], loss=config.loss_function, loss_weights=config.loss_weights)\r\n",
    "\r\n",
    "# The keras summary is stored as a WandB log\r\n",
    "wandb.log({\"summary\": model.summary()})"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "wandb: wandb version 0.12.2 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.12.1<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">major-pine-15</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/sanoli/DanceDanceConvolutionX\" target=\"_blank\">https://wandb.ai/sanoli/DanceDanceConvolutionX</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/sanoli/DanceDanceConvolutionX/runs/2asrzyfa\" target=\"_blank\">https://wandb.ai/sanoli/DanceDanceConvolutionX/runs/2asrzyfa</a><br/>\n",
       "                Run data is saved locally in <code>e:\\Archivos\\Educación\\Posgrado\\Universidad Galileo (UG)\\Trimestre III\\Statistical Learning II\\Proyecto\\DDC Revisited\\wandb\\run-20210927_005803-2asrzyfa</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"DDCX\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Input (InputLayer)              [(None, 5, 15, 80, 3 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Conv2D-1 (TimeDistributed)      (None, 5, 15, 80, 10 640         Input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "MaxPool2D-1 (TimeDistributed)   (None, 5, 13, 78, 10 0           Conv2D-1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "Conv2D-2 (TimeDistributed)      (None, 5, 13, 78, 20 1820        MaxPool2D-1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "MaxPool2D-2 (TimeDistributed)   (None, 5, 4, 26, 20) 0           Conv2D-2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "Flatten (TimeDistributed)       (None, 5, 2080)      0           MaxPool2D-2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Concat-Diff (Concatenate)       (1, 5, 2085)         0           Flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "Dense-1 (Dense)                 (1, 5, 256)          534016      Concat-Diff[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Dense-2 (Dense)                 (1, 5, 128)          32896       Dense-1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "BiLSTM-1 (Bidirectional)        (1, 5, 400)          526400      Dense-2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "BiLSTM-2 (Bidirectional)        (1, 5, 400)          961600      BiLSTM-1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "BiLSTM-3 (Bidirectional)        (1, 5, 400)          961600      BiLSTM-2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "Dense-3 (Dense)                 (1, 5, 128)          51328       BiLSTM-3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "Dense-4 (Dense)                 (1, 5, 64)           8256        Dense-3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "Prob-Step (Dense)               (1, 5, 1)            65          Dense-4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "BPM (Dense)                     (1, 5, 1)            65          Dense-4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "Prob-Left (Dense)               (1, 5, 9)            585         Dense-4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "Prob-Right (Dense)              (1, 5, 9)            585         Dense-4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "Prob-Up (Dense)                 (1, 5, 9)            585         Dense-4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "Prob-Down (Dense)               (1, 5, 9)            585         Dense-4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "Concat-Outs (Concatenate)       (1, 5, 38)           0           Prob-Step[0][0]                  \n",
      "                                                                 BPM[0][0]                        \n",
      "                                                                 Prob-Left[0][0]                  \n",
      "                                                                 Prob-Right[0][0]                 \n",
      "                                                                 Prob-Up[0][0]                    \n",
      "                                                                 Prob-Down[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 3,081,026\n",
      "Trainable params: 3,081,026\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model Training"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Se entrena el modelo \r\n",
    "model.fit(X_train, y_train, epochs=config.epochs, batch_size=config.batch_size, validation_data=(X_valid, y_valid), \r\n",
    "       callbacks=[\r\n",
    "            # Se envían datos a weights and biases\r\n",
    "            WandbCallback(\r\n",
    "                data_type=\"image\",                      # Se generan imágenes en el reporte\r\n",
    "                monitor=\"accuracy\",                     # Monitorea el accuracy como métrica\r\n",
    "                mode=\"max\",                             # Trackea aumentos en accuracy\r\n",
    "                save_model=True,                        # Guardar modelo cuando se alcanza un nuevo máximo en accuracy\r\n",
    "                validation_data=(X_valid, y_valid),     # WandB hace predicciones a medio proceso y las despliega en el dashboard\r\n",
    "            ),          \r\n",
    "       ])\r\n",
    "\r\n",
    "# Se mide la precisión con set pruebas\r\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\r\n",
    "print(f\"Test Error Rate: {round((1-accuracy)*100, 2)}\")\r\n",
    "\r\n",
    "# Se loguean los resultados en WandB\r\n",
    "wandb.log({\"Test Error Rate\" : round((1-accuracy)*100, 2)})\r\n",
    "run.join()\r\n",
    "\r\n",
    "# Se finaliza el \"run\" de WandB\r\n",
    "run.finish()"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.2",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.2 64-bit"
  },
  "interpreter": {
   "hash": "658dc12c475a3a8caebf03b24f414cffa2901ebd330ffd26b9c22f028a90850c"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}